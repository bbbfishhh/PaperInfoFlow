# LLM as Optimizers

---

# 将大语言模型视为优化器

在本章中，我们介绍并讨论了将大语言模型（LLMs）概念化为优化器的现有研究。首先需要指出的是，目前大多数研究集中在第 9.1 式中定义的**提示优化问题**上，而针对具备代理能力的工作流中其他组件的优化仍然是一个新兴的研究领域。接下来，我们将 LLM 优化与经典迭代算法进行类比，并探讨其如何融入现代优化工作流。

---

## 10.1 优化范式

传统的优化方法在目标函数可访问性方面存在差异。我们将其划分为三个主要类别，每类在输入空间上逐渐扩展：

1. **基于梯度的优化**：依赖显式函数梯度。
2. **零阶优化**：在缺乏梯度信息的情况下进行优化。
3. **基于 LLM 的优化**：将自然语言作为优化领域和反馈机制，扩展到结构化和高维的输入空间。

**• 基于梯度的优化**
这类方法假设可以访问梯度信息，并通过迭代方式更新参数。如随机梯度下降（SGD）和牛顿法等技术被广泛应用，但它们要求函数可微，这在离散问题（如提示微调、结构化决策流程）中受限。

**• 零阶优化**
这类方法绕过了对显式梯度的需求，通过函数评估来估算搜索方向，如贝叶斯优化、进化策略和有限差分方法。当梯度无法获得或计算成本过高时，它们非常有效，但仍依赖于数值目标和结构化搜索空间，这限制了其在基于语言的任务中的适用性。

**• 基于 LLM 的优化**
LLMs 借助自然语言在更广泛的解决方案空间中进行优化。它们能够进行结构化推理和类似人类的迭代改进，擅长提示优化、生成自适应工作流，并根据用户反馈逐步提升任务性能。

尽管基于梯度和零阶的方法通常用于数值目标，其核心思想——如迭代改进、搜索启发式、适应性学习等——也为 LLM 优化策略提供了理论基础。基于这些启发，目前涌现出一类基于强化学习的 LLM 优化方法，已成为“慢思维”推理模型的基础。随着这些模型的持续演进，我们预计它们将推动具备代理能力的下一代应用，使 LLM 在复杂环境中展现更强的适应性和战略远见。

---

## 10.2 LLM 优化的迭代方法

部分基于 LLM 的优化方法直接从经典优化理论中汲取灵感，改造其关键组件以应对离散和结构化问题。这些方法的核心特征是**迭代更新步骤**：从多个可能的改进中选择最佳修改以优化目标。例如，使用第 9.1 式中的提示优化目标，一个通用的迭代算法可以表述为：

```
采样：T ∼ D  
评估：L(T; Tp) ← ϕ_eval(ϕ_exe(Q, Tp), T)  
更新：T'p ← ϕ_opt(L(T; Tp))
```

其中的采样与更新步骤根据代理任务而定。在最简单的情况下，比如优化电影评论的二分类指令，L 就是分类准确率。在更复杂的代理工作流中，决策变量可能包括多个阶段的提示、工具选择、代理结构等。这些变量常常具备**组合性质**，例如：从词汇表 V 中选取的字符串集合，或代理角色的所有可能分配。由于穷举所有解通常是不可行的，因此需要设计**近似更新步骤**ϕ\_opt，下面我们详细介绍几种方法。

---

### • 随机搜索（Random Search）

早期的 LLM 优化方法多采用随机搜索来优化自然语言提示。这些方法类似于进化算法，每轮迭代中采样一组候选提示，然后选出性能最好的保留或变异。其通用流程为：

```
采样：T ∼ D  
评估：L(i) ← ϕ_eval(ϕ_exe(Q, Tp^(i)), T), i = 1, ..., M  
更新：保留前 K 个提示：{Tp'^(k)}k=1^K ← ArgTopK_i L(i)  
补充（可选）：{Tp^(j)}j=K+1^M ∼ Mutate({Tp^(k)}k=1^K)
```

这些方法简单易实现、高度可并行，适合于单一提示工作流。在选择上下文示例方面同样表现良好。但每轮迭代需要 O(M) 次并发 LLM 调用，在复杂工作流中成本高昂。

---

### • 梯度近似（Gradient Approximations）

这种方法试图模仿梯度优化，通过历史改进方向逐步更新提示。典型方法如 StraGO 使用中心差分启发式估算下降方向，Trace 则将程序建模为计算图以实现类似反向传播的优化。流程如下：

```
采样：T^(i) ∼ D, i = 1, ..., M  
评估：L(i) ← ϕ_eval(ϕ_exe(Q, Tp), T^(i))  
梯度近似：g ← ∇LLM Agg[L(1), ..., L(M)]  
更新：Tp' ← ϕ_opt(Tp, g)
```

其中 ∇LLM 是抽象的“LLM 梯度算子”，可生成文本形式的改进方向。相比随机搜索，梯度近似方法利用历史信息实现更快收敛，但设计成本更高，例如需使用元提示（meta-prompts）聚合反馈。此外，一些方法尝试直接对“软提示”（soft prompts）进行梯度优化，适用于简单任务但在多步决策中效果不佳。值得关注的是，如 Revolve 等新方法正在探索二阶优化，在多轮优化中利用响应模式的演化趋势以实现更稳定的性能提升。

---

### • 贝叶斯优化与代理建模（Bayesian Optimization & Surrogate Modeling）

尽管上述方法取得了良好效果，但高频率的 LLM 交互带来巨大经济与环境成本，且对噪声敏感。此背景下，贝叶斯优化（BO）成为有吸引力的替代方案：通过代理模型模拟目标函数，提升鲁棒性与效率。其流程如下：

```
采样：T ∼ D  
建议生成：{Tp^(i)}i=1^M ∼ S.Propose  
评估：L(i) ← ϕ_eval(ϕ_exe(Q, Tp^(i)), T)  
更新代理模型：S ← S.UpdatePrior({L(i)}, {Tp^(i)})
```

其中 S 是目标函数的代理模型（如高斯过程），用于根据已有数据生成新的候选提示。MIPRO 使用树结构 Parzen 估计器作为代理模型，PROMST 训练得分预测模型以指导优化。这类方法代表了一种“**摊销式优化**”（Amortized Optimization）思路，通过预训练生成器或得分函数降低后续计算开销，适用于非可微目标，例如 jailbreak 攻击提示生成等。

---

综上所述，LLM 优化策略正从随机搜索、梯度近似，逐步演进到代理建模，覆盖从提示调优、演示选择、工具调用到复杂的多代理系统。未来的发展方向包括引入更高级的优化技术（如二阶方法）、减少成本的摊销式策略，以及在推理阶段有效利用历史轨迹信息。


---

### 10.3 优化超参数

与传统优化方法类似，基于大语言模型（LLM）的方法对影响搜索效率和泛化能力的超参数也高度敏感。在基于梯度的LLM优化器中，一个关键因素是聚合函数 `Agg(·)` 的选择，它决定了如何整合文本反馈以指导提示词的更新。不恰当的选择可能导致关键信息的丢失或迭代优化中的方向偏差。

此外，\[813] 提出了一种“白板”方法，将LLM程序分解为人类可理解的模块。然而，如何设计这些模块化工作流的结构仍缺乏深入探索，这为优化LLM驱动的决策流程带来了挑战。

LLM优化中的超参数常常与数值优化中的类似。例如，**批大小**在其中扮演着关键角色：正如在经典优化中，小批量更新有助于提升稳定性与效率，LLM方法如 TextGrad \[728] 也通过多个生成样本聚合反馈后再进行更新。另一个关键因素是**动量**——在梯度方法中，它通过引入过去的梯度来稳定更新，而LLM优化器也通过利用历史优化结果来持续提升性能 \[728, 813]。尽管数值优化中已有诸多进展，LLM优化器的超参数选择仍主要依赖于启发式策略，通常需要反复试验来调整。

在智能体系统的设计中，超参数在多个组件中广泛存在，例如智能体的角色分配、上下文示例的选择、工具调用的调度等。这些选择都会深刻影响最终性能，但针对这些问题的系统性优化方法仍然不成熟。虽然传统的超参数调优方法（如网格搜索和贝叶斯优化）可以应用于基于LLM的离散工作流中，但由于语言模型输出的不确定性较大，这些方法的计算成本极高。

此外，这些超参数具有组合性特征，例如智能体配置、提示策略与推理结构之间复杂的交互关系，使得穷举搜索变得不可行。近期的研究尝试将智能体工作流嵌入结构化框架中，如有限状态机 \[729]、最优决策理论 \[826] 和博弈论 \[827]，但这些方法往往难以在多样化环境中泛化。

一个有前景的方向是**元优化（meta-optimization）**，即利用LLM优化其自身的超参数与决策策略。例如，LLM优化器可以将过去的决策视为经验，逐步改进其提示策略，类似于深度学习中的“学习型优化器” \[829]。此外，**摊销式方法（amortized approaches）** 通过训练辅助模型预测有效的超参数，从而减少穷举搜索的成本 \[821, 823]。尽管这些方法充满潜力，但也引入了新的挑战，例如在自适应调优中平衡探索与利用，以及保证在不同优化任务中的泛化能力。研究适用于LLM工作流的系统化元优化策略，仍是未来研究的重要方向。

---

### 10.4 在深度与时间维度上的优化

与在静态环境中更新参数的传统优化器不同，LLM是在动态优化工作流，涉及“深度”（单次执行）与“时间”（重复更新）两个维度。

在深度方面，LLM类似于前馈神经网络，按顺序经过多个模块优化工作流——当前大多数基于LLM的优化器遵循这种范式。超过单次执行后，LLM还能进行时间维度的优化，类似于循环神经网络（RNN）或通用Transformer \[830]，通过不断迭代改进决策过程。例如，StateFlow \[729] 利用多轮反馈优化工作流，从而实现动态调整与提升。

虽然这些类比令人信服，但很多传统工程优化技术（如**检查点存储（checkpointing）** \[831] 和**截断反向传播（truncated backpropagation）** \[832]）在LLM优化中仍未被充分研究。我们认为这是一个非常有前景的研究方向，呼应了之前的研究呼吁 \[813]。

---

### 10.5 理论视角

近期研究表明，Transformer本质上执行的是类似优化的计算，这支持了其作为通用优化器的潜力。然而，实际应用与理论理解之间仍存在显著差距。以下简要概述了弥合这一差距的研究进展。

* **上下文学习（In-Context Learning）**
  将Transformer视为优化器的核心视角来自于上下文学习，特别是在少样本学习中。研究 \[733] 证明，Transformer能够在上下文中学习多种回归模型，包括正则化线性模型、决策树和浅层神经网络。在此基础上，后续工作 \[734, 833, 735] 提供了构造性证明，表明Transformer可以实现迭代优化算法，如梯度下降与二阶更新。然而，这些理论模型虽刻画了Transformer的优化能力，但尚未完全解释大规模LLM中在离散输入输出空间中的上下文学习机制。实证分析 \[819, 834, 820] 则试图揭示预训练LLM如何在上下文中泛化。\[834] 提出上下文学习类似于隐马尔可夫模型（HMM）执行隐式贝叶斯推断；而 \[819, 820] 则质疑传统观点，即上下文示例是用于形成假设的新测试样本。上下文学习仍是支撑LLM自我改进与优化的核心“涌现能力” \[835]，但其完整的理论分析仍未建立。

* **机制可解释性（Mechanistic Interpretability）**
  与理论分析并行的是机制可解释性研究，旨在通过识别负责特定行为的子图（也称为“电路”）揭示Transformer的内部计算过程。早期研究在预训练的GPT-2模型中识别了处理特定语言任务的电路 \[836, 837, 838]，更近期的工作则利用稀疏自编码器识别出具有语义意义的特征 \[839, 736, 840, 841]。这些方法在实现前沿LLM的因果与可控行为方面取得了成功，但也揭示了一个副作用：上下文学习能力往往在多示例输入中将有益的泛化能力与潜在有害行为纠缠在一起 \[842]，这给安全、可靠地优化LLM工作流带来挑战。

* **不确定性下的局限性（Limitations Under Uncertainty）**
  虽然在提供上下文信息的前提下，LLM在序列决策中表现出一定能力，但在面对不确定性时仍难以做出最优决策 \[843, 844, 845, 846]。例如，\[826] 发现LLM优化器在适应随机环境方面存在困难，往往难以进行有效探索。这些发现提醒我们在动态或不确定环境中部署LLM优化器时必须谨慎，尤其是在需要探索与稳健决策的场景中。

---

**总结：**
LLM通过整合结构化推理、自然语言处理与上下文学习，重新定义了“优化”的内涵，拓展了其在传统数值方法之外的应用边界。尽管其在结构化搜索空间中展现出强大的经验性能，但诸如上下文学习为何能从标准的基于梯度的训练中“涌现”等理论基础问题，仍有待深入探索。


---

## 一、什么是“超参数”，为什么重要？

* **超参数**就像「游戏设置」里的难度、音量、画质等选项。它们本身不是我们要学的“内容”，而是影响学习过程或搜索效率的「外部开关」。
* 对于大语言模型（LLM）做优化，也有类似“超参数”：

  1. **批大小（batch size）**：一次让模型看多少反馈样本，就像一次给小朋友看几张图来猜画的内容。批大小设得合适，模型学得既稳又快；太小容易“抖”，太大又慢。
  2. **动量（momentum）**：相当于给优化过程加一个“记忆”，让模型沿着之前“好的方向”继续走，不会来回大摆动。
  3. **反馈聚合方式（Agg）**：当模型从多次尝试里得到很多反馈时，要怎样“合并”这些信息？就好比把不同朋友给的建议融合成一个总结，合不好就会误导下一步改进。

> **小白比喻**：想象你在写作文，你每天写几篇（批大小），然后把它们的错别字统计起来（聚合方式），再结合前一天改错的经验（动量）来决定今天重点练哪一项。超参数就是这三件“怎么练”、和“练多少”的设置。

---

## 二、优化要看“深度”和“时间”

1. **深度（Depth）**

   * 就像流水线：模型一次从头到尾过一遍，每个环节负责不同的“检查和改进”（例如先改错别字，再润色语句，再检查逻辑）。
   * 大多数 LLM 优化就是让模型单次执行“流水线”式地改进。

2. **时间（Time）**

   * 模型不止一次跑完流水线，而是「多次」跑，每一次都用上一次的结果继续改进。
   * 类似写作文反复修改：第一稿改一遍，第二稿再改，直到满意为止。每一轮都能更深入地提升质量。

> **小白比喻**：你烘焙一个蛋糕，第一遍放进烤箱；尝一口觉得不够甜再加糖再烤；第二次再尝觉得外皮不够松，就调整温度……这就是“时间维度”上的多轮优化。

---

## 三、LLM 优化背后的理论思考

1. **上下文学习（In-Context Learning）**

   * 模型在“看”到示例（上下文）后，能自己“摸索”出解决问题的方法。
   * 理论家证明，Transformer（LLM 的核心架构）其实能在内部做类似梯度下降、二阶优化的“数学运算”，只不过它是用「文字」来实现这些操作。

2. **机制可解释性（Mechanistic Interpretability）**

   * 研究者像解剖机器一样，把模型内部的“电路”一块块拆开，找出哪些部件在做怎样的判断。
   * 发现优点：能让模型的决策更可控；缺点：有时好的能力和“会出错的习惯”会被捆绑在一起，带来安全风险。

3. **在不确定环境下的局限性**

   * 模型在面对随机、变化的场景时，往往不善于「大胆尝试+稳妥跟进」——很难像人一样在“先试错再改进”之间找到平衡。
   * 这提醒我们，想把 LLM 当成智能决策助手，还要更谨慎地处理探索（尝新）与利用（用已知好方法）之间的关系。

> **小白比喻**：
>
> * 上下文学习就像你看了几道数学题的解题步骤，就能自己推演出类似题目的解法；
> * 机制可解释性就像拆开计算器看里面线路，弄明白每个按钮是怎么把数字加减乘除的；
> * 不确定性局限就像走迷宫：有的小伙伴一进就乱走一气，有的小伙伴只敢走安全路，但都不太擅长“先冒险再修正”的策略。

---

## 四、核心 takeaways（核心收获）

1. **LLM 优化不只是数值计算**：它用「自然语言」当空间、当反馈，能处理文字和结构化任务。
2. **设计超参数、流水线结构、迭代次数**，就像设定学习计划，决定模型“看多少样本”、和“改多少轮”──这些都极其关键。
3. **理论虽在进步，但仍有空白**：我们正努力弄清模型到底是怎么“在头脑中”用文字做优化运算的。
4. **实践需平衡效率与成本、安全与探索**：和调教宝宝一样，既要让它多学多试，又要防止走歪路、消耗过多资源。

---

