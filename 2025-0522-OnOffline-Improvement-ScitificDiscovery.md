#

RAG : 

- Automated Design of Agentic Systems

- xLAM: A Family of Large Action Models to Empower AI Agent Systems

**在线与离线的智能体自我提升**

在自我提升的过程中，智能体将“优化”作为一种机制，用于精炼其各个组成部分，如提示词设计、工作流编排、工具使用、奖励函数的调整，甚至优化算法本身；同时，也将其作为一种战略框架，以确保这些局部的改进能够协调一致地提升整体性能。例如，单独优化奖励函数和提示词设计可能会产生相互冲突的结果，但通过战略性的优化方式，可以协调这些优化行为，使其保持一致性并最大化整体效果。

我们将智能体的自我演化分为两种主要范式：**在线自我提升**和**离线自我提升**。此外，我们还探讨了融合这两种方式的**混合优化策略**，以实现最大化的效率与适应性。

---

### 11.1 在线智能体自我提升

**在线自我提升**指的是实时优化过程，即智能体基于即时反馈动态地调整其行为。这一范式通过不断优化关键性能指标（如任务成功率、延迟、成本和稳定性），确保智能体能够对不断变化的环境保持敏感并作出响应。

在线自我提升在需要高度动态适应的应用场景中尤为有效，如：实时决策制定、个性化用户交互以及自动推理系统。

在线自我优化的关键策略可分为以下四类：

---

#### **迭代反馈与自我反思**

这类方法（如文献 \[48, 67, 72, 70, 847, 47]）致力于让智能体能够迭代地评估并改进自己的输出。

* **Reflexion \[48]**、**Self-Refine \[67]** 和 **Tree of Thoughts \[72]** 引入了“自我批判循环”，使模型能在实时中发现错误并提出修改建议。
* **ReAct \[70]** 则将“思维链（chain-of-thought）推理”与“执行（acting）”结合，在接收到外部反馈后能迭代修改步骤。
* 还有其他方法，如通过**自洽性（self-consistency）\[78]** 筛选最连贯的解答，或使用**过程奖励模型（PRM）**（如 Lightman 等人 \[847] 所提出）从多个候选解中选择最优解。

这些方法共同减少了错误的累积传播，支持快速适配环境，无需额外的离线微调周期。

---

#### **多智能体系统中的主动探索**

此类方法（如文献 \[626, 848, 627, 152]）强调在多智能体系统中主动探索与动态搜索新模式或工作流优化策略。

* **MetaGPT \[626]**、**CAMEL \[848]** 和 **ChatDev \[627]** 展示了多角色或多智能体生态系统，这些系统在实时交互中持续提供反馈，互相优化彼此的贡献。
* 类似地，**HuggingGPT \[152]** 使用一个中心的大模型控制器协调多个专用模型（托管在 Hugging Face 平台上），动态分派任务并收集反馈。

这些协作策略进一步体现了智能体之间的在线更新如何渐进式地优化集体结果。

---

#### **实时奖励塑形**

与依赖固定或离线定义奖励机制的方法不同，一些框架（如文献 \[731, 91, 105, 849]）引入了**即时反馈信号**，不仅用于纠错，还用于动态调整内部奖励函数和策略。
这使得奖励机制具备了自适应能力，可以在**性能、计算成本与响应延迟**之间进行权衡，并根据用户交互过程灵活优化，从而实现动态的奖励校准。

---

**动态参数调优**
在这一类别中，智能体能够自主地实时更新其内部参数（包括提示模板、工具调用阈值、搜索启发式策略等），通常采用**无梯度或近似梯度方法**。这些更新旨在优化计算效率和决策准确性，使智能体能够无缝适应不断变化的环境上下文。
例如，**自引导优化（Self-Steering Optimization, SSO）\[850]** 通过在迭代训练中自主生成偏好信号，消除了人工标注的需求，在保持训练过程在线（on-policy）的同时，确保了信号的准确性。

**在线自我提升**塑造了一个持续演化的智能体框架，在任务执行过程中嵌入学习能力，促进了实时适应性提升、以用户为中心的优化以及强大的问题解决能力。

---

### **11.2 离线智能体自我提升**

与在线优化相对，**离线自我提升**依赖结构化、批量化的优化过程。这一范式通过计划好的训练周期，借助高质量的精选数据集，系统性地提升智能体的泛化能力 \[851, 667, 852, 853, 854]。
与在线方法不同，离线优化可以使用计算密集型的策略，例如**批量参数更新与微调**、**元优化**以及**系统化奖励模型校准**等。

---

#### **批量参数更新与微调**

在此类别中，智能体通过监督学习或强化学习（RL）技术进行大规模微调，跨多个训练轮次优化在大数据集上的整体性能。
常见方法还包括整合**检索增强生成（Retrieval-Augmented Generation, RAG）\[740, 741]**，以提升上下文理解与长期记忆能力。这类方法帮助智能体优化检索策略，从而增强其在大规模知识库中的推理能力。

---

#### **智能体组件的元优化**

离线训练不仅限于提升任务性能，还包括优化**优化算法本身**。
元学习（Meta-learning）策略可用于优化超参数，甚至动态重构整个优化流程，已在多个研究中展现出良好效果 \[731, 91]。
这些元优化方法让智能体能够为新问题域发现最有效的学习参数。

---

#### **系统化的奖励模型校准**

离线环境有助于精确校准奖励模型，常使用**层次化或列表式奖励集成框架**（如 LIRE \[855]）来通过基于梯度的奖励优化，使智能体行为与长期目标保持一致。
这种校准过程确保奖励函数能真实反映现实任务的复杂性，从而减少偏差、增强泛化能力。

---

离线优化的结构化特点能够构建一个**稳健的智能体基线模型**，并在部署前对其性能进行微调，以优化稳定性、效率与计算成本。
离线训练对于需要**可预测性能保障**的关键任务应用至关重要，能够实现高保真度的模型精炼。

---

### **11.3 在线与离线自我提升的比较**

在线优化与离线优化具有**互补优势**，各自在自我提升的不同方面表现出色。

* **在线优化**在动态环境中表现优异，能够依赖**实时反馈持续适应**，特别适用于需要即时响应的应用场景，如交互式智能体、实时决策系统、强化学习环境等。但其频繁更新可能带来系统不稳定或“漂移”，因此需设计机制来缓解性能退化问题。
* 相对地，**离线优化**更强调结构化、保真度高的训练过程，依靠预先收集的数据集，在部署前确保系统的鲁棒性和稳定性。通过批量训练、微调与元优化等计算密集型方法，离线优化具有更强的泛化能力与长期一致性。然而，其灵活性不如在线学习，面对新场景时若无额外训练可能难以高效适应。

---

**表格 11.1：在线优化与离线优化策略在智能体自我提升中的对比**

| 特征    | 在线优化          | 离线优化               |
| ----- | ------------- | ------------------ |
| 学习过程  | 基于实时反馈进行持续更新  | 在计划好的训练阶段进行批量更新    |
| 适应能力  | 高，能够动态调整      | 低，仅在重新训练后才能适应变化    |
| 计算效率  | 对增量更新更高效      | 因批量训练较为耗费资源        |
| 数据依赖性 | 依赖实时数据流       | 依赖精心整理的高质量数据集      |
| 过拟合风险 | 连续学习降低了过拟合的风险 | 若训练数据不具多样性则过拟合风险较高 |
| 稳定性   | 频繁更新可能导致不稳定   | 在可控训练条件下更稳定        |

尽管这两种方式各具优势与权衡，但**现代智能系统越来越多地采用混合优化策略**，融合两者优势：利用离线训练的稳定性，同时引入实时适应能力，使智能体能在动态环境中不断优化，同时保持长期稳健性。

---

### **11.4 混合优化方法（Hybrid Approaches）**

由于在线和离线方法各有局限，许多现代系统采用**混合优化策略**，将结构化的离线优化与响应式的在线更新相结合，实现智能体的持续增量式增强。

混合优化明确支持自我提升，使智能体能在以下**彼此关联但阶段分明的流程中自主评估、适应与改进自身行为**：

---

#### • 离线预训练（Offline Pre-Training）

在这一基础阶段，智能体通过在精选数据集上进行广泛的离线训练，获取稳健的基础能力。这一阶段建立了智能体自主执行任务所需的基本技能，如推理与决策。
例如，Schrittwieser 等人提出的框架 \[856] 展示了如何通过离线预训练系统性地增强智能体初始能力，确保后续在线优化建立在稳固基础之上。

---

#### • 在线微调以实现动态适应（Online Fine-Tuning for Dynamic Adaptation）

智能体在实际运行中主动评估自身表现，识别不足，并基于实时反馈动态调整策略。这一自适应微调阶段直接体现了自我提升范式，允许智能体对其工作流和行为进行实时优化。
如 Decision Mamba-Hybrid（DM-H）\[857] 就展示了智能体如何高效适应复杂、不断变化的场景。

---

#### • 定期离线整合以促进长期提升（Periodic Offline Consolidation for Long-Term Improvement）

在定期的离线整合阶段，智能体系统性地融合并巩固在在线交互中获得的改进成果。该过程确保增量式的在线能力提升被有效地纳入核心模型中，维持长期的稳定性与效果。
Uni-O4 框架 \[858] 即是这一过程的典型代表，它展示了如何实现离线知识整合与在线适应性提升之间的无缝衔接。

---

**混合优化策略**通过结构化的离线学习与积极主动的实时在线适应之间的有机结合，明确支持了智能体的自主、持续演进。这种循环式优化方式赋予智能体**即时响应能力**与**稳定的长期优化能力**，特别适用于复杂的现实应用场景，如自动驾驶机器人、个性化智能助手与交互式系统等。

---

**第 12 章
科学发现与智能进化**

在前几章中，我们主要从技术视角讨论了智能体系统的演进，重点在于如何开发能够有效执行人类传统任务的系统。然而，一个根本而重要的问题依然存在：这些智能体是否能推动一个自我维持的创新循环，进而促进智能体自身的演化以及人类的进步？

科学知识的发现是智能体自我进化的一个极具说服力的例证，因为它帮助智能体以可持续的方式适应世界。能够以不同程度的自主性且以安全方式进行科学知识发现的智能体，也将在推动人类技术创新方面发挥重要作用。本节我们将综述利用智能体工作流进行自主科学发现的最新进展，并讨论通向完全自主、自我演化智能体的技术成熟度。在这一背景下，智能体的目标是发掘、验证并整合数据、洞见与原理，以推进对自然现象的客观科学理解。与改变世界不同，这些智能体更像是“科学家 AI”（Scientist AI）\[859]，致力于更好地理解自然，并协助人类拓展知识边界。

我们首先定义“知识”与“智能”的概念，以明确本章讨论的核心，然后介绍智能体与科学知识交互的三种典型场景。我们还将重点介绍已经取得成功的、自我增强的智能体在理论、计算与实验科学研究中的应用实例。最后，我们对当前面临的挑战进行总结，并展望未来。

---

### 12.1 智能体在科学知识发现中的智能表现

“知识”传统上被定义为“被证实的真信念（justified true belief）”，这一概念可追溯至柏拉图 \[860]，后由埃德蒙·盖梯尔（Edmund Gettier）进一步发展，他指出知识必须来源于可靠的认知过程——尽管至今对知识的精确定义仍存在争议 \[862]。在本章中，我们将“科学知识发现”定义为：收集数据与信息，以验证或证伪针对科学问题提出的合理假设的过程。为了讨论智能体在科学知识发现中的能力，我们首先从信息论的角度，提出一个评估智能体智能水平的通用框架。

---

#### 12.1.1 基于 KL 散度的智能测度

智能体的智能可以通过它对未知信息的预测概率分布与真实概率分布之间的 KL 散度（Kullback-Leibler divergence）来衡量。在人工智能和科学哲学领域中，一个长期目标就是形式化地定义“理解世界”的含义。从 Jaynes 将概率论视为不确定性推理的扩展逻辑 \[863]，到 Parr 等人基于自由能原理（free energy principle）提出将智能定义为最小化模型与世界之间的差异 \[864]，诸多理论均趋向于一个共同主题：**智能行为源于对不确定世界的准确预测**。

Clark \[344] 指出，智能体通过预测与误差修正不断与世界互动，从而减少“意外”（surprise）；Chollet \[865] 强调，智能应反映在**任务适应中的技能习得效率**上。这些观点共同指出，智能体的本质在于构建**可预测、可适应的模型**，我们将在此基础上，提出一个概率框架，以此形式化推理与知识获取之间的联系，并用于评估科学发现中的不同智能体。

---

在科学知识发现的具体语境下，智能体的主要目标是：**在有限数据的条件下推断自然世界中未知的规律**。在此过程中，世界 $W$ 由与科学问题相关的数据集 $x = \{x_1, x_2, ..., x_n\}$ 构成。每个数据点 $x_i$ 可能存在相关性。比如，在语言模型的文本生成任务中，$x_i$ 表示一个有意义的命题片段，$x$ 则是由这些命题构成的连贯文本；此时，“世界”就是所有命题的集合。

设 $\theta$ 表示智能体世界模型 $M^{\text{wm}}_t$ 的参数（如 Transformer 模型的权重）。给定参数 $\theta$ 和数据 $x$，智能体预测的概率为 $P_\theta(x)$。我们假设在科学发现场景下，智能体的目标是：**尽可能准确地描述自然世界**，即构建能够预测未被探索现象的世界模型。智能体智能水平的衡量方式为：

$$
D_0(\theta) = \sum_{x \subseteq W} P_W(x) \log \frac{P_W(x)}{P_\theta(x)} \tag{12.1}
$$

此处的 KL 散度 $D_0(\theta)$ 描述了现实分布 $P_W(x)$ 与模型预测分布 $P_\theta(x)$ 的差异。在假设检验的语境下，若我们对 $P_W(x)$ 采样 $N$ 次，并与 $P_\theta(x)$ 的结果进行比较，误判的概率将以 $e^{-ND_0(\theta)}$ 的速度衰减 \[866]。也就是说，**D 值越小，说明模型预测越贴近现实，智能水平越高**。

举例：两个材料合成智能体的目标是判断一种化合物 CaFe₂(PO₄)₂O 是否可合成。Agent 1 完全猜测：

* $P_{\theta_1}(x_1) = P_{\theta_1}(x_2) = 0.5$，
* 则 $D_0(\theta_1) = \log 2$。

Agent 2 利用第一性原理计算得出该材料为最低能态，具有稳定性：

* 预测 $P_{\theta_2}(x_1) > 0.5 > P_{\theta_2}(x_2)$，
* 所以 $D_0(\theta_2) < D_0(\theta_1)$，表明 Agent 2 对真实世界有更准确的理解。

---

现在，设智能体已通过某些实验获得一部分数据 $x_K$，而剩下的未知数据为 $x_U$。定义已知知识空间 $K$、未知信息空间 $U$，满足：

* $x_K \subseteq K$,
* $x_U \subseteq U$,
* 且 $K \cup U = W$。

比如在语言生成中，输入提示 $x_K$ 就是已知部分，输出文本 $x_U$ 是预测部分。我们用以下公式衡量智能体基于已知数据进行预测的智能水平：

$$
D_K(\theta, x_K) = \sum_{x \subseteq U} P_W(x|x_K) \log \frac{P_W(x|x_K)}{P_\theta(x|x_K)} \tag{12.2}
$$

在实际中，智能体的知识存储于其记忆模块 $M^{\text{mem}}_t$，即

* $x_K = K = M^{\text{mem}}_t$,
* $U = W \setminus M^{\text{mem}}_t$。

因此，智能体的智能定义为：

$$
IQ^{\text{agent}}_t \equiv -D_K(\theta, M^{\text{mem}}_t) = -\sum_{x \subseteq U} P_W(x|M^{\text{mem}}_t) \log \frac{P_W(x|M^{\text{mem}}_t)}{P_\theta(x|M^{\text{mem}}_t)} \tag{12.3}
$$

换句话说，**智能体的智能水平 $IQ^{\text{agent}}_t$** 由其记忆 $M^{\text{mem}}_t$ 与世界模型参数 $\theta$ 决定。图 12.1 中提供了一个示意图。在 $t=0$ 时，若记忆中缺乏与科学任务相关的信息，则 IQ 主要取决于模型的零样本预测能力（类似流动智能，fluid intelligence）\[867]。随着更多知识被集成进记忆，IQ 越来越依赖知识增强后的预测能力（类似结晶智能，crystallized intelligence）\[868]。

---

以下是上述英文内容的中文翻译：

---

**图12.1：智能体智能与知识发现的示意图。**
智能体的智能程度通过预测结果与真实世界概率分布之间的KL散度 $D_K$ 来衡量，随着时间 $t$ 推移，智能体在其记忆 $M^{\text{mem}}_t$ 中积累数据，其智能也从“流体智能”（对新问题的零样本预测）演化为“晶体智能”（在学习后的知识增强预测）。给定 $M^{\text{mem}}_t$，KL散度的演化在世界模型参数空间 $\Theta$ 内变化，如实线所示的 $\theta_1$ 和 $\theta_2$ 所示。参数空间 $\Theta$ 的表达能力限制通过包络线 $D^{\text{min}}_{K, \Theta}$ 来表征。不同的知识扩展策略，如 $1M^{\text{mem}}_t$ 和 $2M^{\text{mem}}_t$，会影响 $D^{\text{min}}_{K, \Theta}$，如虚线所示。

---

### 12.1.2 智能增长的统计性质

从统计意义上说，智能体的智能是其所获得知识量的非减函数。大致而言，智能 $IQ^{\text{agent}}_t$ 衡量的是智能体所掌握的知识量以及其在学习了记忆 $M^{\text{mem}}_t$ 后运用该知识的能力。直观来看，如果智能体在某一时刻 $t$ 获得了更多信息（即扩大了 $M^{\text{mem}}_t$，缩小了未知区域 $U$），其智能应当提升。

为了理解这一过程，考虑一个小区域 $\Delta \subseteq U$，并研究将该区域的数据集 $x_\Delta$ 添加至 $M^{\text{mem}}_t$ 的效果。设 $U = U' \cup \Delta$，其中 $U'$ 表示剩余未知世界。则智能体在 $t+1$ 时刻的智能为：

$$
IQ^{\text{agent}}_{t+1} \equiv -D_K(\theta, M^{\text{mem}}_t \cup x_\Delta) = - \sum_{x' \subseteq U'} P_W(x'|M^{\text{mem}}_t \cup x_\Delta) \log \frac{P_W(x'|M^{\text{mem}}_t \cup x_\Delta)}{P_\theta(x'|M^{\text{mem}}_t \cup x_\Delta)}
\quad (12.4)
$$

由于难以直接比较 $IQ^{\text{agent}}_t$ 与 $IQ^{\text{agent}}_{t+1}$，我们可计算 $IQ^{\text{agent}}_{t+1}$ 关于 $x_\Delta$ 的期望值：

$$
\sum_{x \subseteq \Delta} P_W(x|M^{\text{mem}}_t) IQ^{\text{agent}}_{t+1} = IQ^{\text{agent}}_t + \sum_{x \subseteq \Delta} P_W(x|M^{\text{mem}}_t) \log \frac{P_W(x|M^{\text{mem}}_t)}{P_\theta(x|M^{\text{mem}}_t)} \quad (12.5)
$$

第二项为以 $M^{\text{mem}}_t$ 条件下的相对熵（KL散度），总是非负，因此平均而言，智能体的智能随时间非减增长。
值得注意的是，智能体可以利用新获得的知识优化世界模型参数 $\theta$，进一步提高 $IQ^{\text{agent}}_{t+1}$。

有趣的是，智能增长的期望值取决于真实分布 $P_W(x|M^{\text{mem}}_t)$ 与模型预测分布 $P_\theta(x|M^{\text{mem}}_t)$ 之间的差异。换言之，如图12.1所示，当新观测结果更出乎意料时，智能增长的速率越快。这一现象表明，**科学家型智能体**是一类**以好奇心驱动**的智能体，优先进行探索而非利用现有知识，以扩展知识边界并深入理解自然。与那些以实现既定目标为导向的智能体不同，好奇心驱动的智能体即使没有外部奖励也能自主学习，从而突破人类设定的搜索空间，探索未知领域。这种潜力也强调了为这类智能体配备基本感知和行动工具的重要性，以支持其探索新知识领域。

---

### 12.1.3 智能演化策略

已知信息的扩展策略决定了智能体智能演化的速度。对于给定的知识库 $M^{\text{mem}}_t$，参数 $\theta$ 可以在由世界模型架构 $M^{\text{wm}}_t$ 所决定的参数空间 $\Theta$ 中进行优化。最优智能体是使 KL 散度最小化（即智能最大化）的那个：

$$
\theta^*_{K,t} \equiv \arg\sup_\theta IQ^{\text{agent}}_t = \arg\inf_\theta D_K(\theta, M^{\text{mem}}_t) \quad (12.6)
$$

$$
D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t) \equiv D_K(\theta^*_{K,t}, M^{\text{mem}}_t) \quad (12.7)
$$

这里，$D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t)$ 表示在给定模型族 $\Theta$ 下，学习了 $M^{\text{mem}}_t$ 后剩余未知的最小值，量化了模型表达能力的限制。如图12.1所示，$D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t)$ 构成了 $D_K(\theta, M^{\text{mem}}_t)$ 函数族的包络线。

该包络线随知识库扩展单调不增。智能体的行动方式由 $M^{\text{wm}}_t$ 决定，通过选择行为 $a_t \in A$ 来获得信息并扩展知识库（参见表1.2）。

在知识发现过程中，可以采用不同策略扩展 $M^{\text{mem}}_t$。最优扩展策略是使 $D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t)$ 降低最快的策略。例如，图12.1展示了两种策略：

* **策略1：** $1M^{\text{mem}}_t$，代表随机探索；
* **策略2：** $2M^{\text{mem}}_t$，基于假设驱动方法，即先提出关于问题机制的假设，再设计实验验证或证伪该假设。

实际中，实验科学家更倾向于使用假设驱动方法，因为该方法可在资源受限条件下，最大程度地减少 $D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t)$，效率高于随机探索。
因此，策略 $2M^{\text{mem}}_t$ 可比 $1M^{\text{mem}}_t$ 更快降低 $D^{\text{min}}_{K,\Theta}$。

总体而言，知识发现过程是一个迭代过程：不断优化模型参数 $\theta$ 向最优 $\theta^*_{K,t}$ 靠近，并理性地扩展 $M^{\text{mem}}_t$，以加速 $D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t)$ 的下降。
理想状态为“认知完备”，即 $D^{\text{min}}_{K,\Theta}(M^{\text{mem}}_t) = 0$，预测与现实无差异。但实际中常存在“发现极限”，即便 $D^{\text{min}}_{K,\Theta}$ 接近零，也无法完全为零，这源于 $\Theta$、动作空间 $A$ 及其他设计限制。
要实现低发现极限，需要设计：

* 自适应的世界模型架构；
* 高效的知识扩展策略；
* 足够丰富的动作空间。

---

### 12.2 智能体-知识交互

科学知识主要包括三种形式：

* **观察知识**（如实验测量、计算结果），
* **方法知识**（如实验方法、计算技术、操作协议），
* **理论知识**（如理论体系、自然法则、预测模型）。

只要这些知识能以影响未知信息的概率分布 $P_\theta(x_U | M^{\text{mem}}_t)$、减少 KL 散度 $D_K(\theta, M^{\text{mem}}_t)$、促进决策为目的被加工利用，它们就能推动科学理解的发展。

原则上，已有研究表明，**外部科学知识**在提升智能体推理与决策能力方面具有显著效果。然而本节更关注的是：**智能体如何自主发现和利用知识以增强自身能力。**

科学知识发现流程通常包括：

* 假设生成、
* 协议制定、
* 实验与计算、
* 数据分析、
* 推导结论、
* 修正假设，

并形成循环迭代流程。

一个具备感知、学习、推理和行动能力的智能体，理论上可以自主驱动该流程。例如，它可通过 API 接口与物理仪器交互获取科学知识，并迭代更新其知识库（如图12.2）。
智能体将使用所获取的知识更新其心智状态 $M_t$，以便在与世界 $W$ 的交互中做出更优决策。接下来的内容将重点展示三种智能体发现科学知识、增强自身能力的典型场景。

---

以下是该段英文的中文翻译：

---

**图12.2：用于可持续自我进化的闭环知识发现。**
智能体旨在通过假设生成与检验、数据分析与推理推导，迭代增强自身的智能水平 $IQ^{agent}_t$。当与物理世界 $W$ 交互时，智能体会以显式或隐式的形式预测未知信息的分布 $P_\theta$，生成假设；它会采取动作 $a_t$ 来检验假设，观察实验结果 $o_t$，并根据对真实世界分布 $P^W$ 的感知更新自身信念。当不与世界交互时，智能体则从已有数据和前提中提炼知识，直接更新心理状态 $M_t$。该图灵感来自文献\[864]中的图2.3与2.5。

---

### 12.2.1 假设生成与检验

假设生成与检验（见图12.2）是智能体在自主科学发现中的关键应用，有潜力推动“跳出框架”的创新\[749]。本质上，假设生成指的是构建可能主导数据分布的潜在规则，这些数据分布可能来自单个观测或大型数据集，通常与尚未被观察到的科学现象有关。

根据卡尔·波普尔爵士的观点，一个科学假设必须具备可证伪性\[875, 876]；在本节中，我们将能够经受证伪检验的假设定义为“被证实的真实假设”\[877, 860]。科学家通常通过实验来验证或推翻假设，一个假设若能涵盖广泛数据并具较高真实性，便更有价值。

为解决科学问题，智能体会基于其心理状态 $M_t$（仅包含对部分可观测世界 $W$ 的不完全信息）提出一个或少量高价值假设。经过实验或计算验证后，被证实的真实假设将转化为指导性知识，从而扩展记忆状态 $M^{mem}_t$，迅速减少 $D^{min}_{K,\Theta}(M^{mem}_t)$。因此，生成并验证高价值假设可快速推动知识发现，提升智能体 IQ。

在这一过程中，智能体使用学习函数 $L$，将从假设检验中得到的观测结果 $o_t$ 转化为知识，并更新其心理状态 $M_t$。

生成具有物理意义的假设是关键步骤。智能体通常使用大型语言模型（LLM）结合协同架构和领域知识来进行假设生成\[878]。Si 等人\[742]进行了一项大规模人类研究，涵盖100多位自然语言处理研究者，发现LLM生成的创意在新颖性上得分高于人类专家（p < 0.05），但在可行性上略逊一筹。

Ghafarollahi 等人\[743]开发了 SciAgents，用于生成和完善材料科学的假设，从而揭示材料的基本机制、设计原则以及意外特性。SciAgents 基于大规模本体知识图谱，从感兴趣概念间采样一条可行路径，形成相关假设，并扩展成完整研究方案，含详细检验方法与标准。系统配有两个专用代理，用于审阅、批评和改进该假设，但尚未涵盖实际实验检验。

Su 等人\[879]与 Baek 等人\[880]同样提出使用团队协作（如讨论与代理批评者）来生成新颖有效的科学假设。Gower 等人\[881]则提出 LGEM+ 系统，使用一阶逻辑框架描述生化路径，自动生成2094个候选假设，用于改进酵母菌 S. cerevisiae 的代谢模型。

假设只有在经由计算或实验验证后才能转化为知识。Lu 等人\[745]提出 AI Scientist 系统，实现完全自动化科学发现，涵盖三个子领域（扩散建模、基于Transformer的语言建模、学习动力学）。系统能生成研究创意、编写代码、进行实验、可视化结果、撰写论文，甚至模拟同行评审。例如，它提出“自适应双尺度去噪可提升扩散模型的生成质量”这一假设，并通过四个二维数据集的图像生成实验加以验证。

Schmidgall 等人\[746]开发 Agent Laboratory，能自主执行整个科研流程（含文献综述、计算实验与报告撰写），并在五个计算机视觉与自然语言处理问题上实现了平均人类评分3.2/5的实验质量。

Tiukova 等人\[744]开发 Genesis 系统，自动控制1000个微型生物反应器、进行质谱表征、访问结构化信息库，并基于实验结果更新系统生物学模型。Genesis 每天能执行1000轮以假设驱动的闭环实验流程。在酵母菌的双相代谢模型研究中，该系统在基因和相互作用数量上提升显著（+45%、+147%），并推动对癌症、免疫系统、衰老的理解。

Gottweis 等人\[749]提出 AI 共科学家系统，能自动生成并完善新研究假设，并在三个生物医学领域（药物重定位、新靶点发现、抗药性机制）中进行了体外验证。

被发现的知识会增强智能体的心理状态，如 $M^{mem}_t$、$M^{wm}_t$、$M^{rew}_t$。Tang 等人\[747]开发 ChemAgent，利用动态自更新记忆机制 $M^{mem}_t$ 提升化学推理能力。它针对问题集提出假设答案、与真实结果对比，并模拟现实中的假设检验过程，正确答案被存储为知识。该方法在四个化学推理数据集上（来自 SciBench）将GPT-4表现提升至46%。

Wang 等人\[884]提出 MOLLEO，使用语言增强的进化优化方法，在 $M^{mem}_t$ 中迭代提出药物分子假设，评估其可药性与活性，并优化候选分子。Jia 等人\[885]则开发 LLMatDesign，结合假设驱动的结构生成和自更新记忆机制，设计理想带隙与最低形成能的无机光伏材料。

Sim 等人\[748]提出 ChemOS 2.0 系统，协调化学自驱实验室（SDL）中的闭环操作。该系统集成 *ab initio* 计算、实验流程编排与统计算法，用于高性能材料发现。案例研究中，它使用贝叶斯优化器 Altas 预测有机激光材料（如 BSBCz 衍生物）的光学性能，并推荐成功率较高的实验候选分子，实验数据再用于更新世界模型 $M^{wm}_t$，提升未来实验预测准确性。

Hysmith 等人\[886]发表观点指出：在SDL中，奖励函数设计在自我进化流程中至关重要。虽然智能体可有效解决模拟环境下的POMDP问题（如游戏），但现实环境中往往因缺乏明确奖励函数而表现不佳。许多科研任务在实验周期末并无可量化奖励，或需权衡多个目标。此时，新知识的发现可用作强化 $M^{rew}_t$ 的资源，指导假设探索与实验数据收集。

---

以下是该部分内容的中文翻译：

---

### 12.3 技术准备度与挑战

代理体的自我进化——进而推动人类知识的进步——源于其在创新循环中的早期成功。这个循环包括生成有意义的假设、设计实时测试协议、协调各种实验与计算工具、分析数据、推导影响，并进行自我反思。然而，要实现完全自主的自我进化仍面临重大挑战，当前三项基础能力的技术成熟度（Technology Readiness Levels, TRLs）尚未达到要求：真实世界交互、复杂推理、以及先验知识的整合。要实现自驱动创新的良性循环，还需要进一步的技术进步。

---

### 12.3.1 真实世界交互的挑战

代理体与真实世界的交互主要通过应用程序接口（API）进行。尽管已有大量案例表明代理体在调用各种API方面能力强大 \[891]，但在自主知识发现上仍存在显著瓶颈：缺乏可让代理体直接在物理实验室执行任务的API。物理API（即能直接控制实验设备的接口）远不如计算API丰富，因为开发这些接口需要大量时间、专业知识与成本投入。

虽然目前已有一些自主实验室展现出潜力，但这些系统仍处于开发早期阶段（通常为TRL 4–6），难以直接复制或扩展。因此，要构建新的系统或将其应用拓展到更多科学领域，仍需进行大量定制开发以满足特定领域的需求，并需要专业人才的参与。

实现真实世界交互的两个关键任务是：**操作实验室设备**与**在设备之间转移样品**。要实现实验流程的无缝衔接，必须在物理硬件与实验样品之间建立紧密的整合。然而，大多数实验设备最初是为人工操作而设计的，使其可被代理体访问，需要跨学科的巨大努力，包括机器人学、电气工程、机械工程和软件编程等领域。

随着自驱动实验室（Self-Driving Labs, SDL）的兴起，人类操作的设备正通过API向代理体可访问系统转变。在执行复杂实验的自主实验室中，常采用两种并行但互补的方式来将硬件集成入代理体系统中。这两种方式都是模块化的、可重配置的且具有价值，但都需要持续且专注的开发。

#### 方法一：通过设备改造实现API集成

该方法通过为每个设备配备专用的机械适配器与输入/输出控制器，使其能从中央控制PC接收并执行指令。例如，为实现无机材料的固态合成与结构表征，A-lab 已部署了16种设备，用于自动化执行粉末配药、加热和衍射等实验任务 \[892]。该方法可最大限度提高设备利用率、优化空间与资源，并实现定制化工具，使实验室成为高度整合的系统。然而，这种方式代价高昂、耗时长、且需要专业知识来原型设计或改装设备以实现自动化。为简化这一过程，大语言模型（LLM）被用于辅助接入多种工具，例如 CACTUS（Chemistry Agent Connecting Tool-Usage to Science）\[893]。

对于小型研究团队而言，**云实验室或科学工厂** \[894]是一种更易获取的替代方案。在这种模式下，设备工程的责任由单个实验室转移至专门的用户设施或商业服务提供商。例如，Boiko 等人 \[895] 展示了一个自主化化学研究代理体 Coscientist，其能在 Emerald Cloud Lab \[896] 中执行 Suzuki 与 Sonogashira 反应。但云实验室通常仅提供一组针对常规流程优化的预构建设备，因此对于需要设备定制的研究者而言，整合非标准工具可能需要较长的谈判与开发过程。

#### 方法二：机器人操作实验设备

这种方式利用移动机器人或机械臂来操作现有设备并转移样品。在很多情况下，机器人可在仅进行轻微改动（如添加特定执行器、夹持器或支架）的前提下，与仪器交互。例如，Dai 等人 \[750] 使用移动机器人探索合成化学。在他们的自主实验室中，移动机器人可在空间上分隔的合成与分析设备之间建立物理连接，实现样品自动运输与处理。原则上，机器人可执行实验室中人类研究者的全部操作。然而，目前的机器人系统仍需人类预先编程来绘制实验室布局、定义移动轨迹及登记设备位置。在面对突发或适应性情况时仍存在挑战，因为预编程无法预见所有可能的实验状态。**实时学习与自适应操作**是当前研究的热点，仍需进一步技术突破。从长期来看，具身人工智能（Embodied AI）\[897] 有望增强机器人的学习能力，使代理体能更快速适应新环境与工具。

这两种方式还可组合使用。例如，Vescovi 等人 \[894] 定义了一种模块化实验室机器人架构，可将高级命令翻译为针对多种机器人装置与实验设备的具体操作，并将机器人系统与AI驱动的发现架构中的其他元素（如高性能计算）相连接 \[898]。该架构已被应用于生物与物理科学实验的自动化 \[899]。同样，Fernando 等人 \[900] 将兼容ROS2的机器人整合至 Bluesky 实验编排框架中。Lo 等人 \[901] 倡导开发低成本的“节俭双胞胎”设备，用于替代昂贵设备，降低实验门槛并实现普惠访问。

---

### 12.3.2 复杂推理的挑战

一个基本的哲学问题是：由LLM驱动的代理体是否真的能进行“推理”？按定义，大语言模型是通过预测下一个词元（token）来生成输出的，这种机制与人类推理本质上是不同的。但从结果导向的角度来看，这些输入输出系统表现出类似推理的能力，因为其生成的输出与参考系统相比具有意义 \[902]。

然而，无论采取哪种视角，这种能力仍然不完美——尤其是在处理复杂的逻辑和数值问题时，而这些问题对科学知识发现至关重要。

代理体与LLM在困难推理任务上表现不佳。例如，Glazer 等人 \[903] 提出了 FrontierMath，这是一个涵盖现代数学主要分支的大量原创高难度题目的基准测试。对多种最先进LLM驱动代理体（如 o1-preview、GPT-4o、Claude 3.5、Grok 2 Beta 和 Gemini 1.5 Pro）的评估显示，**在整个基准测试中无一模型正确率超过2%**。Chen 等人 \[873] 提出的 ScienceAgentBench 基准，测试了102项来自44篇同行评审论文的科学发现任务，OpenAI o1的成功率仅为42.2%。Chollet \[865] 提出了ARC挑战，用于评估LLM在无需记忆和外部知识的前提下是否具备抽象归纳推理能力。即便在精心设计的提示下，GPT-4o的成功率也仅为19%，远低于人类平均水平（约75%）\[904, 905]。Zhu 等人 \[906] 将AI智能划分为四个等级：L1（调解争议）、L2（审查评论）、L3（评审论文）和 L4（撰写论文），并认为目前最强的LLM代理体仅接近L2级别。

为了增强代理体的推理能力，研究人员提出了诸如“思维链”（chain-of-thought）\[907]、“思维树”（tree-of-thoughts）\[72]等方法。尽管方法不断推陈出新，但要实现科学研究中可靠的因果推理，还需进一步提升推理能力。

此外，LLM在**数值和符号处理方面的能力也很有限**。例如，GPT-4与GPT-3.5在执行诸如12345 × 98765这类复杂算术时常出错，或难以将IUPAC化学命名准确翻译为分子结构图 \[908, 697]。为克服这些限制，通常采用外部工具（如符号计算器）辅助推理 \[753]。但这种手段并未解决LLM自身对数值理解的根本缺陷，这可能在科学推理中造成风险。

此外，Yu 等人 \[909] 发现：即使引入外部工具，LLM在解决化学问题上也未必比基础模型表现更好。例如，在合成预测这类特化任务中，工具增强能显著提升性能；但在更通用的化学考试题目中，由于没有直接解决方案，**代理体自身调动多个知识点推理的能力更为关键**。

综上所述，**构建科学研究助手代理体的评估体系**至关重要，Cappello 等人 \[910] 就此进行了深入讨论。

---

### 12.3.3 先验知识整合的挑战

先验知识是高级智能的关键因素。如第12.1节所述，代理体的先验知识（记作 Mmemₜ）有助于降低 DK(θ, Mmemₜ)，从而提升代理体的智能水平 IQagentₜ。人类科学发现往往能在较小的数据集上取得突破，得益于丰富的先验知识。

尽管最先进的LLM几乎训练于所有公开文本数据（包括网站、书籍等），涵盖了大多数常识和公开专业知识，但**让代理体真正无缝整合人类全部知识**仍面临巨大挑战。

至少存在三类知识未能充分覆盖于LLM的预训练数据中：

1. **受限访问或未公开的知识**：如非开放获取的论文、行业数据、失败实验数据等，虽对专业洞察极具价值，但公共模型往往无法访问。

2. **经验性知识**：专家基于经验作出的启发式决策往往极为有效，特别是在缺乏现有数据的新问题场景中。然而，许多专家经验并未被记录为文本数据。

3. **情境性知识**：如化学反应的安全规范或设备操作细节等现实条件相关知识，虽对实际应用至关重要，但很难出现在训练数据中。

此外，将不同来源的知识融合也面临冲突信息的调和问题。例如，OpenAI 的 Deep Research 系统 \[912] 虽可主动聚合信息并进行多步推理，在Humanity’s Last Exam与GAIA基准上表现优异，但仍难以准确区分权威信息与谣言，且在**可信度校准方面存在问题**，经常夸大其置信水平 \[912]。

因此，构建一种能评估不同知识片段证据等级的系统（如量化可靠性、核查引用等）\[913]，或将成为高效知识整合的关键。

---

以下是这段内容的中文翻译：

---

### 12.2.2 协议规划与工具创新

制定实验协议和优化工具使用的能力，使智能体能够在自主发现循环中解决复杂的科学难题。如第9.4节所述，智能体可以系统地评估和改进其选择、调用和集成可用工具的方法，甚至可以开发适用于特定任务需求的新工具。虽然优化协议和工具使用并不会直接降低 $DK(\theta, M_t^{mem})$，但它们可以提高执行效率和效果，从而更有效地细化未知信息的概率分布 $P_\theta(x_U | M_t^{mem})$，进而加速知识发现。在此过程中，智能体利用推理函数 $R$ 将不断更新的新知识状态 $M_t$ 转化为现实世界中的动作 $a_t$，以实现更高效、更快速的假设验证（见图 12.2）。

调度和编排现有工具的选择与重组至关重要。科学实验通常依赖多种仪器来分析反应产物，决策很少只依赖于单一测量结果。为了有效使用必要的仪器而不浪费资源和时间，智能体需要学习如何以集成和自适应的方式使用工具。Dai 等人\[750] 设计了一个模块化工作流，整合了移动机器人、自动合成平台和各种表征仪器，实现自主科学发现。他们在三个领域展示了该系统的应用：结构多样化化学、超分子主客体化学和光化学合成。移动机器人通过合成-分析-决策循环模仿人类实验策略，自动决定后续流程步骤。它会选择适当的仪器，例如用于合成的 Chemspeed ISynth 平台，用于化学峰信号质量分析的超高效液相色谱-质谱联用仪（UPLC-MS），以及用于追踪起始材料到产物的化学转化的台式核磁共振波谱仪（NMR）。

在单个实验室之外，工具的编排对去中心化和异步科学发现也至关重要。Strieth-Kalthoff 等人\[751] 展示了五个材料科学实验室之间的闭环整合，这些实验室分布在三个大洲，推动了去中心化和大众化的科学发现。这五个实验室各有所长，例如不列颠哥伦比亚大学专注于连续择优结晶，而九州大学擅长薄膜制造与表征。研究人员使用基于云的实验规划器，持续从新数据中学习，并有效地在五个实验室中优先安排最具信息价值的实验，从而发现了 21 种有机固态激光的新型先进材料。

此外，智能体还可以优化现有工具，甚至创造新工具以增强自身能力。Swanson 等人\[752] 开发了“虚拟实验室”（Virtual Lab），这是一个由人工智能驱动的研究环境，用于设计和实验验证新型 SARS-CoV-2 纳米抗体。在该虚拟实验室中，AI 智能体在团队会议中进行科学讨论，在个人会话中执行专门任务。其中一项关键任务是开发用于纳米抗体结合设计的辅助工具\[887]，包括：（1）一个序列分析工具，利用 ESM 蛋白语言模型的对数似然比对候选点突变进行排序\[888]；（2）一个结构评估工具，从 AlphaFold-Multimer 的预测中提取界面 pLDDT 分数，用于近似抗体-抗原结合亲和力\[889]；（3）一个基于 Rosetta\[890] 的能量评估工具，用于量化纳米抗体变体与病毒刺突蛋白之间的结合强度。借助这些 AI 生成的工具，虚拟实验室成功发现了两种对 JN.1 或 KP.3 SARS-CoV-2 变体具有增强结合力的纳米抗体，同时保持了对原始病毒刺突蛋白的强亲和力。

---

### 12.2.3 数据分析与蕴含推导

尽管大多数知识发现过程依赖于生成假设并在现实世界中验证（其中观测 $o_t$ 是关键），但大量知识也可以通过内部行为获得，例如迭代推理和深入思考，这在理论学科中尤为常见。例如，欧几里得几何中的所有定理都可以从五条公理中推导出来，但这些定理在被推导出来之前并不显式存在于心智状态中。即使具备了所有必要的前提（如欧几里得五大公设），某个假设的真实概率可能仍无法直接得出。然而，通过演绎和归纳推理从已知前提出发进行蕴含推导，可以用来证明或证伪假设，从而降低 $DK(\theta, M_t^{mem})$ 并提升智能体智力 $IQ^{agent}_t$（见图 12.2）。在这一过程中，智能体使用认知函数 $C$，利用先前的心智状态 $M_{t-1}$ 和内部动作 $a_t$ 来推导新知识并更新为 $M_t$。

**演绎推理**允许通过逻辑进行知识推导。Trinh 等人\[753] 开发了 AlphaGeometry，用于基于现有欧几里得平面几何定理进行新定理的前向推导。AlphaGeometry 利用神经语言模型在几何问题中构建辅助点，并结合专用符号引擎以穷尽方式推导新的真实陈述，从而扩展已知真理的闭包集合。通过这种方式，系统在辅助构造与符号推理之间交替运行，进一步挖掘逻辑蕴含。在一组包含 30 道最新奥赛水平问题的测试集中，AlphaGeometry 解出了其中的 25 题，远超此前最好方法的 10 题，接近国际数学奥林匹克（IMO）金牌得主的水平。

**归纳推理**通过模式识别与统计学习进行知识推导。Liu 等人\[754] 提出了“AI 科学家团队”（TAIS），该系统模拟数据科学家的角色，以实现高效数据分析。TAIS 将复杂的数据分析问题分解为多个计算任务，包括编程、自我审查和回归分析，以从复杂数据集中提取有意义的洞见。在用于识别疾病预测基因的任务中，TAIS 在包含 457 个基因问题的基准数据集上达到了 45.73% 的整体成功率。理想情况下，提取出的洞见应具有逻辑上的合理性；否则，就必须被舍弃，以确保只有准确的发现才能被安全整合进智能体的心智状态。然而，由于数据覆盖范围的限制及分析算法的局限性，也可能产生“幻觉性”洞见，因此亟需可靠的数据分析器和推理工具，以避免过度分析。

---




