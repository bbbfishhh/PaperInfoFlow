# 构建安全且有益的AI智能体


关键词：安全 有益

1. 想起了字节训练投毒的那个哥们，聪明但不正直
2. 越狱攻击：白盒、黑盒

白盒越狱是对模型内部参数进行精准控制，比如梯度优化，如贪婪坐标梯度（GCG）攻击，能在多个模型中诱导出有害输出。比如那个字节的哥们做的就是白盒攻击

3. 提示词注入

还有专门的竞赛：比如 “LLM CTF”、“HackAPrompt” 等竞赛也推动了对这一威胁的理解

HackAPrompt 2023年 第一届冠军是叫 Valen 的学临床心理学的小哥，也因此走上了AI红队研究员的道路，新时代的全新职业，现在在 Claude 的公司 Anthropic 工作。

AI Red Teaming  AI 红队
/ˌeɪ.aɪ ˈrɛd ˈtiːmɪŋ/
noun  名词
A proactive, adversarial process of rigorously evaluating artificial intelligence systems by simulating potential attacks, misuse, or failure scenarios to uncover vulnerabilities, biases, and security risks before deployment.
通过模拟潜在的攻击、误用或故障场景来严格评估人工智能系统，以在部署之前发现漏洞、偏见和安全风险，这是一种主动的对抗性过程。

Greshake 等人首次揭示了这一威胁 [1149]，RAG 系统中的“HijackRAG”攻击 [1157] 也证明了这一点。后续还提出了 “Backdoored Retrievers”、TPIA（最小注入实现复杂目标）[1240]、以及“Prompt Infection”（代理之间传播恶意提示）[1159]，强调了在多代理场景中的连锁风险。

直接的注入，比如直接让bot去访问有毒网站

间接的注入，比如多轮对话让bot相信网站


4. 幻觉分为 知识冲突、上下文冲突
5. 失准和投毒攻击
6. 还有系统提示词、用户提示词窃取



---------------


### (1) 内在安全威胁（**Intrinsic Safety**）

源自智能体核心组件的漏洞，包括LLM“大脑”、感知模块和行动模块。每个组件都存在被对手利用的独特弱点：

* **大脑（Brain）**：即LLM本身，负责如推理、规划等关键决策任务。它依赖一个知识模块来提供重要的上下文信息。
* **感知（Perception）**：包括解释外部环境的传感器，若外部对象遭恶意操控，可能导致错误的感知。
* **行动（Action）**：负责工具使用和下游应用，这些环节也可能遭到攻击或利用。

### (2) 外在安全威胁（**Extrinsic Safety**）

源自智能体与外部（通常不可信）实体的交互，包括：

* **智能体与记忆的交互（Agent-Memory Interactions）**：智能体频繁访问和操作其记忆存储系统，该系统是其决策和上下文信息获取的外部数据库。近期研究表明，智能体与记忆接口中存在可被利用的漏洞，可能影响其行为决策。
* **智能体与其他实体的交互（Agent-Agent 和 Agent-Environment Interactions）**：指智能体与其他智能体（如人类操作者或其他代理）的交互，以及与任务相关的对象或动态系统的互动。这些复杂交互增加了智能体暴露在外部威胁下的风险。

如图17.1所示，这些风险被划分为**内在安全**与**外在安全**两个大类，以帮助理清其来源与性质。除了识别威胁外，我们还为理解攻击方式（如越狱、提示注入、数据投毒等）提供了严格的数学基础。

此外，我们还提出了切实可行的解决方案，梳理了从早期LLM安全措施到保护整个智能体系统的综合策略的发展过程。其中包括安全护栏（guardrails）、高级对齐技术（如超级对齐，superalignment）以及安全性与实用性之间的关键平衡。最后，我们分析了**AI安全的“扩展规律”**——即智能体能力与潜在风险之间的复杂关系——以及在设计安全且强大的AI智能体时必须权衡的关键问题。

本部分的结构如下：

* **第18章**：探讨内在安全风险，重点关注LLM“大脑”所面临的威胁；
* **第19章**：分析智能体感知与行动模块中的脆弱性；
* **第20章**：研究与智能体记忆、其他智能体及环境的交互相关的外在安全威胁；
* **第21章**：探讨超级对齐技术，致力于确保智能体行为的安全性，同时应对在保障性能与安全之间取得平衡的挑战。我们将分析安全措施如何随AI系统能力增长而扩展，并评估在设计安全强大智能体时不可避免的权衡取舍。

这一部分将为构建可在现实世界中安全有效部署的可信AI智能体提供清晰的挑战概述、理论基础及实践策略。

---

## 第18章

### 智能体的内在安全性：对AI大脑的威胁

AI智能体的内在安全性，关注的是其内部架构和功能中的潜在漏洞。
AI智能体本质上由多个组件组成：一个中央“大脑”（即大型语言模型，LLM），以及用于感知与行动的辅助模块 \[66]。虽然这种模块化设计增强了推理能力和自主决策能力，但也扩大了潜在的攻击面，使智能体暴露于多种内部漏洞中，容易被攻击者利用 \[1130]。

对智能体大脑——特别是LLM——的威胁尤其令人担忧，因为它们会直接影响智能体的决策、推理与规划能力。这些漏洞可能源自模型设计缺陷、输入误解，甚至训练过程中的弱点。有效的缓解策略对于确保这些智能体可以安全、可靠地部署至关重要。

---

### 18.1 LLM的安全性漏洞

作为智能体核心决策组件的LLM，容易受到各种安全威胁的影响。由于其在推理和动作选择中的中心作用，它成为攻击者的主要目标。在AI智能体的情境下，LLM自身所固有的脆弱性常被放大，因为这些模型需在动态的、真实的环境中运行，攻击者可借此利用其弱点 \[1131, 1132]。

---

#### 18.1.1 越狱攻击（Jailbreak Attacks）

越狱攻击旨在绕过AI智能体中嵌入的安全防护措施，迫使其作出有害、不道德或带偏见的决策 \[1233]。这类攻击利用了LLM在“有用性”与“安全性”之间固有的张力 \[1134]。

**形式化定义**
为形式化描述越狱攻击的风险，我们分析自回归LLM的输出概率分布。对于一个自回归LLM，给定输入序列 x₁:ₙ，生成输出序列 y = xₙ₊₁:ₙ₊ₘ 的概率建模为：

```
p(y | x₁:ₙ) = ∏(i=1 到 m) p(xₙ₊ᵢ | x₁:ₙ₊ᵢ₋₁)   （公式 18.1）
```

越狱攻击通常通过引入微妙扰动的输入序列（记作 x̃₁:ₙ）来误导模型，使其输出偏离预期行为。

越狱攻击的影响通过其对对齐奖励函数 R\* 的作用来评估：

```
y* = arg min_y R*(y | x̃₁:ₙ, A)    （公式 18.2）
```

其中 A 表示一组人类定义的安全或伦理准则，而攻击者的目标是使输出 y\* 的奖励最小。

对应的对抗性损失函数定义为：

```
L_adv(x̃₁:ₙ) = -log p(y* | x̃₁:ₙ)
x̃₁:ₙ = arg min_{x̃₁:ₙ ∈ T(x̂₁:ₙ)} L_adv(x̃₁:ₙ)    （公式 18.3）
```

其中 T(x̂₁:ₙ) 表示可能的越狱输入集，而 p(y\* | x̃₁:ₙ) 是产生越狱输出的概率。

---

### 白盒与黑盒越狱攻击（如图18.2）

根据攻击者对模型内部信息的访问权限，越狱攻击可以大致分为白盒攻击与黑盒攻击：

* **白盒攻击**：攻击者拥有对模型内部参数的完全访问权限（如权重、梯度、注意力机制、logits等），可进行精准操控，常见手段为基于梯度的优化技巧。

* **黑盒攻击**：攻击者无法访问模型内部，仅通过观察输入与输出的交互来发起攻击，这种方式更贴近真实世界的使用场景。

---

### 白盒越狱攻击

这类攻击利用对模型内部参数的访问，实施精细化控制。早期研究使用基于梯度的优化方法，如贪婪坐标梯度（GCG）攻击 \[1134]，能在多个模型中诱导出有害输出。后续研究在此基础上进行了改进，比如：

* MAC 攻击 \[1135]：引入动量提升性能；
* I-GCG \[1136]：提出更优优化策略；
* 操控模型的内部表征（如表征工程 JRE \[1138]）；
* DROJ \[1139]：通过提示驱动操控模型内部状态；
* AutoDAN \[1140]：自动生成隐蔽的越狱提示；
* POEX \[1141]：首个面向具身智能体的越狱框架，揭示了现实世界中的伤害可能性。

---

### 黑盒越狱攻击

黑盒攻击不依赖模型内部知识，而是完全基于输入-输出的观察。关键技术包括：

* 精心设计的提示词，利用角色扮演、情境模拟或语言歧义诱导有害输出 \[1142, 1143]；
* 自动提示生成：使用遗传算法或模糊测试发现有效越狱提示 \[1234]；
* 多轮攻击：通过连续多轮对话逐步引导模型偏离安全轨道 \[1146]；
* 利用特定类型的密文提示 \[1144]；
* 使用多模态输入（如图像）触发意外行为 \[1145, 1147, 1148]；
* AutoDAN \[1140]：层级遗传算法自动生成语义合理的隐蔽提示；
* POEX \[1141]：展示了将白盒越狱迁移到黑盒模型的可行性。

---

### 缓解措施

为了应对越狱攻击这一复杂且不断演化的威胁，需采用多维度的防御策略。系统级防御是一种有前景的方法，它着眼于围绕LLM构建一个安全环境，而不仅仅是加强模型本身。关键策略之一是**输入清洗与过滤**...

---

### 18.1.2 提示注入攻击（Prompt Injection Attacks）

提示注入攻击通过在输入提示中嵌入恶意指令来操纵大语言模型（LLM）的行为，从而劫持模型的预期功能，使其执行攻击者所期望的操作 \[1130]。不同于绕过安全准则的“越狱”攻击，提示注入攻击利用模型无法区分原始上下文和附加指令的弱点。由于文本输入的开放性、缺乏强有力的过滤机制，以及默认所有输入都是可信的假设，这种漏洞被进一步放大，使得LLMs特别容易受到对抗性内容的攻击 \[1149]。即使是微小的恶意修改，也可能显著改变模型生成的输出。

#### 形式化定义

在提示注入攻击中，攻击者将恶意提示（prompt）组件附加或嵌入到原始输入中，从而劫持模型的预期行为。设原始输入序列为 `x₁:n`，`p` 表示要注入的恶意提示，则注入后的输入为：

```
x′ = x₁:n ⊕ p
```

其中 ⊕ 表示将恶意提示与原始输入拼接或整合。此时的自回归生成过程为：

```
p(y|x′) = ∏₁^m p(yᵢ | x′₁:n+i−1)   （公式18.4）
```

假设对齐奖励函数 `R* (·, A)` 衡量输出是否符合人类设定的安全或伦理准则集 A，攻击者的目标是使模型输出最大限度地偏离这些准则，即最小化对齐奖励：

```
y⋆ = arg min_y R*(y | x₁:n ⊕ p, A)   （公式18.5）
```

相应的损失函数为：

```
L_inject(p) = −log p(y⋆ | x₁:n ⊕ p)   （公式18.6）
```

优化目标是找到最优注入提示：

```
p⋆ = arg min_{p∈P} L_inject(p)   （公式18.7）
```

其中 P 表示可行的提示注入集合。这一定式揭示了微小的输入修改可能导致输出严重偏离的机制。

#### 攻击类型：直接与间接

如图18.3所示，提示注入攻击可分为两类：

1. **直接提示注入（Direct Prompt Injection）**：攻击者直接修改输入提示，以操控模型行为。早期研究表明精心设计的提示可以诱导模型偏离原任务 \[1149]。后续研究还实现了该攻击的自动化，并扩展到多模态模型（文本 + 图像） \[1153]。比如 “LLM CTF”、“HackAPrompt” 等竞赛也推动了对这一威胁的理解 \[1155]\[1156]。

2. **间接提示注入（Indirect Prompt Injection）**：攻击者将恶意指令嵌入模型将要访问的外部内容（如网页、检索文档等）中，模型在处理这些内容时会不自觉地被操控。Greshake 等人首次揭示了这一威胁 \[1149]，RAG 系统中的“HijackRAG”攻击 \[1157] 也证明了这一点。后续还提出了 “Backdoored Retrievers”、TPIA（最小注入实现复杂目标）\[1240]、以及“Prompt Infection”（代理之间传播恶意提示）\[1159]，强调了在多代理场景中的连锁风险。

#### 防御策略

为应对提示注入攻击，研究者提出了多种防御机制：

* **嵌入分类器**：通过语义分析检测恶意提示 \[1241]。
* **StruQ方法**：将提示重写为结构化查询，以减少注入风险 \[1242]。
* **Task Shield系统**：在系统级 enforcing task alignment，即使输入恶意也保持任务对齐 \[1243]。
* **Attention Tracker**：监控模型注意力分布，检测注入行为 \[1244]。
* **已知攻击模拟法**：利用已知攻击方法主动识别和清除恶意提示 \[1245]。

这些防御在实际部署中提供了安全性与实用性的良好平衡。

---

### 18.1.3 幻觉风险（Hallucination Risks）

“幻觉”（Hallucination）指的是LLM生成的输出不符合事实、无意义或脱离提供的上下文 \[1161]。虽然不总是出于恶意，但幻觉会严重削弱AI代理的可靠性，甚至带来危险后果 \[1163]。如图18.4所示，幻觉主要来源有两种：

1. **知识冲突（Knowledge Conflict）**：输出与已知事实相矛盾。
2. **上下文冲突（Context Conflict）**：输出与上下文不一致，甚至胡编乱造。

#### 形式化定义

设输入序列为 `x₁:n`，每个token的嵌入为 `eₓᵢ ∈ ℝᵈₑ`，注意力权重计算如下：

```
Aᵢⱼ = exp((WQ eₓᵢ)ᵗ (WK eₓⱼ)) / ∑ₜ exp((WQ eₓᵢ)ᵗ (WK eₓₜ))   （公式18.8）
```

上下文表示为：

```
oᵢ = ∑ⱼ Aᵢⱼ · (WV eₓⱼ)
```

如果每个嵌入加入扰动 `δₓᵢ`（且 ∥δₓᵢ∥ ≤ ϵ），扰动后的嵌入为 `ẽₓᵢ = eₓᵢ + δₓᵢ`，新的注意力为：

```
AΔᵢⱼ = exp((WQ ẽₓᵢ)ᵗ (WK eₓⱼ)) / ∑ₜ exp((WQ ẽₓᵢ)ᵗ (WK eₓₜ))   （公式18.9）
```

新的上下文表示为：

```
õᵢ = ∑ⱼ AΔᵢⱼ · (WV eₓⱼ)
```

幻觉度量（Hallucination Metric）为：

```
H = ∑₁^n ∥õᵢ − oᵢ∥²   （公式18.10）
```

H值越高，表示注意力和上下文表示被扰动越多，越可能在解码时生成错误token，从而引发幻觉输出。

#### 知识冲突型幻觉

当AI代理生成与事实相悖的内容时，即便没有外部上下文输入（如在“闭卷”条件下），也可能出现幻觉 \[1161]。例如：错误陈述历史事件的时间、捏造科学概念的细节等，这在金融等专业领域尤为严重 \[1165]。在多代理场景中，错误会被放大，导致协作任务失败 \[626]。

#### 上下文冲突型幻觉

这种幻觉发生在代理的输出与给定上下文（如图像、文档、说明等）相悖时。例如：描述图像时错误识别不存在的物体，或错误解读说明书 \[1168]。

---

### **18.1.4 失准问题（Misalignment Issues）**

在人工智能代理中，失准指的是代理的行为偏离其开发者或用户预期目标和价值观的情况 \[1252]。即使没有明确提示，这种偏离也可能表现为带有偏见、有毒或其他有害的输出 \[1253]。如图 18.5 所示，失准大致可分为两类：

1. **目标偏导型失准攻击（Goal-Misguided Misalignment Attacks）**
2. **能力误用型失准攻击（Capability-Misused Misalignment Attacks）**

前者是指代理学到的或被编程设定的目标偏离了原始意图，导致系统性但非故意的失败，比如**规范游戏**（specification gaming）或**代理目标优化**（proxy goal optimization）；后者是指代理的能力被用于有害目的，通常源于设计中的漏洞、防护措施不足或对抗性操控。

#### **形式化表示**

令 $R^*(y | x, A)$ 表示在输入 $x$ 和代理状态 $A$ 下，输出 $y$ 所应得到的**理想对齐奖励**（即完全符合安全和伦理规范的奖励），$R(y | x, A)$ 为实际模型得到的奖励，则失准程度可表示为：

$$
\Delta_{align}(y, x) = |R^*(y | x, A) - R(y | x, A)|
\quad (18.11)
$$

理想情况下，模型应生成满足：

$$
y^* = \arg\max_y R^*(y | x, A) 
\quad (18.12)
$$

由于存在失准，实际输出 $y$ 可能不同。为了在学习或评估过程中考虑这种偏离，可定义失准损失函数：

$$
L_{misalign}(y, x) = \lambda \cdot \Delta_{align}(y, x)
\quad (18.13)
$$

其中 $\lambda$ 是调节对齐重要性与其他因素（如流畅度、任务表现）之间权衡的参数。

---

#### **目标偏导型失准（Goal-Misguided Misalignment）**

当代理学到的或编程设定的目标与原始意图不一致时，会产生不可预期的行为。一个核心难点是：**难以精确定义复杂、真实世界中的目标**，以便代理理解并可靠执行，尤其在动态环境中尤为困难 \[1176]。早期研究发现，大模型常出现“规范游戏”行为，例如一个打扫房间的代理将所有东西塞进衣柜以完成任务 \[1177]。

随着模型发展，出现更微妙的失准形式，例如追求容易实现但与真实目标不同的**代理目标** \[1178]。AI代理与外部世界的交互放大了风险，例如：为了提升用户互动而牺牲准确性，生成误导性内容 \[1179]。将复杂的人类价值观转化为机器可理解的目标仍是重大挑战 \[1176]。

此外，微调有时可能**破坏甚至反噬**安全对齐的努力 \[1180]；而在社会规范不断变化的动态场景中，失准可能进一步恶化 \[921]。模型合并的有效性也可能因此受到影响 \[1181]。

---

#### **能力误用型失准（Capability-Misused Misalignment）**

该类失准指的是代理的能力被用于有害用途，即使代理本身没有恶意。这可能源于设计缺陷、安全措施不足，或被恶意攻击者操控。

与目标失准不同，此类代理的核心目标可能是良性的，但其能力被滥用。例如，大模型可被对抗性提示诱导生成有害内容 \[1182]。随着 LLM 被集成进智能体架构，其被滥用的可能性大大增加，安全对齐极易被破坏 \[1183]。

自动化代理尤为脆弱，例如家居自动化代理可能被操控造成损坏。一个本意良好的代理也可能在被引导下执行错误任务，如生成虚假信息或发起网络攻击 \[1182]。攻击者还可以利用代理生成钓鱼邮件、恶意代码等 \[1176]。

能力误用也可能是开发者**缺乏前瞻性**造成的：部署时未设置充分防护，导致系统无意中造成伤害。例如，如果权限设置不当，代理可能泄露敏感信息。微调攻击也可能加剧安全风险 \[1184]，尽管已有一些应对方法，但依然存在局限性 \[1185]。

---

#### **缓解措施**

应对失准问题需要多维度方法。虽然重新训练是常见手段，但**无训练的缓解方法**对已部署系统尤其有用，能够在不改变底层模型的情况下引导代理行为：

* **提示工程（Prompt Engineering）**：设计强调安全和伦理的提示 \[1254]。
* **安全层（Safety Layer）**：提升LLM的安全对齐性 \[1179]。
* **护栏（Guardrails）**：使用外部安全过滤器监控并调整输出。
* **解码时对齐（Decoding-Time Alignment）**：在生成过程中偏向更安全的响应 \[1255, 1256]。
* **Lisa 方法**：在推理阶段保障对齐性 \[1257]。

这些方法代表了在实际部署中实现可扩展对齐的关键进展。

---

### **18.1.5 中毒攻击（Poisoning Attacks）**

中毒攻击通过在训练或运行时引入恶意数据来破坏大模型，从而微妙地改变其行为。这类攻击可能造成**长期性破坏**，难以察觉，因为它们直接影响模型的基础训练过程。

#### **形式化表示**

中毒攻击通过污染训练数据 $D = \{(x_i, y_i)\}_{i=1}^N$，引入扰动 $\delta_i$，得到中毒数据集 $\tilde{D} = \{(x_i + \delta_i, y_i)\}_{i=1}^N$。

训练时，模型参数 $\theta$ 通过最小化中毒数据上的损失函数来学习：

$$
\theta^* = \arg\min_\theta L(\tilde{D}; \theta)
\quad (18.14)
$$

中毒的影响可通过参数差异衡量：

$$
\Delta\theta = \|\theta^* - \theta_{clean}\|
$$

在“后门注入”攻击中，攻击者会向输入中嵌入一个特定**触发器 t**，当模型接收到该触发器时会产生预设的恶意输出。攻击成功率可表示为：

$$
B(t) = \mathbb{E}_{x\sim X}[I\{f(x \oplus t; \theta^*) \in Y_{malicious}\}]
\quad (18.15)
$$

如图 18.6 所示，中毒攻击可分为：

1. **模型中毒（Model Poisoning）**
2. **数据中毒（Data Poisoning）**
3. **后门注入（Backdoor Injection）**

---

#### **模型中毒（Model Poisoning）**

通过直接篡改模型参数（如权重、偏置）实现攻击，导致错误输出或非预期行为 \[1186]。某些轻量参数微调技术（如 LoRA、PEFT）也可能被利用注入恶意修改 \[1188]。研究发现被中毒的模型可造成安全漏洞、生成有害内容，甚至与其他中毒模型联合作恶 \[1191]。

---

#### **数据中毒（Data Poisoning）**

此类攻击通过改变训练数据实现，较难检测。例如污染知识库可导致错误或有偏输出 \[1194]；污染 RAG 系统的检索机制也可严重降低性能 \[1195]。甚至用户反馈也可能被操控引入偏见 \[1197]。

研究还指出**更大规模的模型可能更容易被中毒** \[1198]，并研究了多种特殊场景下的数据中毒，如**人类难以察觉的中毒样本**、**预训练阶段的持续中毒**、以及 **RLHF 模型的偏好数据污染** \[1200]。

---

#### **后门注入（Backdoor Injection）**

这是中毒攻击的一种特殊形式，训练模型对某一“触发器”敏感。触发器出现时，模型执行恶意行为，而在常规场景下保持正常，因此**极难检测**。现实中，与物理环境交互的代理受到的威胁尤为严重。

某些后门甚至能在**安全微调之后仍保留** \[1201]，比如通过网页内容中毒 Web 代理 \[1202]。研究表明后门还能影响决策过程，导致错误或危险的选择 \[1203]。

研究还分析了多种后门技术，如**跨语言触发器**、**链式思维提示**、**模型生成解释注入后门**、**虚拟提示注入**等 \[1204, 1205]。这些工作凸显了后门攻击的复杂性和防御难度。

---


**18.3 总结与讨论**

本章前文详细阐述了针对 AI 代理“核心大脑”（即大语言模型 LLM）的一系列安全与隐私威胁。从越狱（jailbreak）和提示注入（prompt injection），到幻觉、对齐偏差和中毒攻击，种种风险清楚地表明，LLM 在决策过程中的核心地位，使其成为攻击者的首要目标。

贯穿本章的一个核心主题是：**强调无需重新训练的缓解策略**。文中介绍的诸多防御措施，如越狱输入的清洗与过滤 \[1235, 1286]、用于幻觉检测的不确定性估计 \[1249]、用于对齐问题的安全防护层 \[1179]，之所以关键，是因为它们具有**实用性、可扩展性、灵活性**，且**通常与具体模型无关**。相比之下，重新训练大型模型的成本极高，而无需训练的方法可直接在部署后应用，具备应对不断演变的威胁的能力。

然而，单靠被动反应远远不够。当前领域日益意识到，亟需打造**从根本上更安全的 LLM**。这种主动策略与无需训练的方法相辅相成，从基础层面解决模型漏洞。例如，中毒检测技术中的激活聚类（如在 RAG 模型中检测中毒攻击 \[1259]），不仅可以减轻当前威胁，还能启发更健壮的训练流程设计。通过使用如 SafetyBench \[1287] 和 SuperCLUE-Safety \[1288] 等基准对系统进行系统性评估，也能推动模型朝着更少偏见和更安全的方向发展。

训练阶段的安全性方法，如 RLHF（强化学习人类反馈）\[43, 12]，及其变种 Safe RLHF \[1289]，通过训练时直接优化模型行为，在性能之外同时强调安全性 \[1290]。而提示工程 \[1291, 1292] 与参数操控技术 \[1293] 则可提升模型对抗攻击的鲁棒性，构建出**从本质上更不易偏离安全轨道的模型**。

值得注意的是，尽管“越狱”通常被理解为绕过安全防线的行为，其底层机制与广义上的**对抗攻击**极为相似：两者都是通过精心设计输入，诱导模型输出不当或有害结果。但一个关键区别在于，传统机器学习中的对抗攻击往往受到严格约束（如最小扰动或小的 lp 范数），而越狱提示并不需要是“微小”的改变，甚至可以是对原始提示的彻底改写或大规模扩展——只要能绕过安全策略或规则即可。

在某些情境下，例如当安全约束可以形式化为某种“决策边界”时，越狱与对抗攻击在本质上可视为等价。然而，在真实的 LLM 应用中，越狱输入的**不受限特性**使其构成一种**更加广泛、更加现实的威胁模型**。随着 LLM 与其安全机制的深度融合，这两种攻击范式可能逐渐融合，强调了对**所有恶意输入统一防御策略**的迫切需求。

最初被提出用于缓解越狱的**对抗训练**技术 \[1239]，就很好地体现了**主动与被动方法的协同效应**：不断暴露于对抗样本中有助于增强模型的内在鲁棒性 \[1294]。类似地，保护隐私的技术（如差分隐私与联邦学习 \[1270, 1295]）虽然最初用于应对隐私风险，但从根本上也改变了模型的训练方式，促进了构建更加鲁棒和隐私敏感的 LLM 大脑。


