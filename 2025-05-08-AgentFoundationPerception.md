# Perception

1. OpenAI 2021 年提出 CLIP，文本与图像的对齐与检索。
还有 OpenAI 的 DALL·E 系列 [563, 564, 502] 从 2021 年至 2023 年在该领域做出了重要贡献，DALL·E 3 能够实现对生成图像的细粒度语义控制。

发现大家用 coze 搭建图片流的时候，用 gpt4o挺多的。

所以实际上的生图里面，DALLE 是很强的，而 Gemini 则是上下文长，和多模态理解强，而生成图片和视频不强。（之前出的是图片一个prompt ps）

2. 




## 7.1 人类与人工智能感知对比

感知是智能的根基，是人类和人工智能智能体与世界互动的接口。尽管人类通常认为感知仅包括五种传统感官——视觉、听觉、味觉、嗅觉和触觉，但现代神经科学揭示出一个更加丰富的感官体系。

保守估计认为人类拥有大约10种感官，更全面的观点列出了约21种，而有些研究者则提出人类可能拥有多达33种独立的感知模式 \[546, 547]。除了熟知的感官之外，人类还具备复杂的内部感知能力，例如前庭觉（平衡感）、本体感受（身体位置感知）、温度感知以及痛觉，使人类能够更加精细地与环境互动。

人类感官对特定的物理信号高度敏感：例如，人眼可感知波长约在380–780纳米之间的电磁波，而人耳则能感知频率在20 Hz到20 kHz之间的声音 \[548]。这些感知模式使人类能够轻松地执行诸如语言交流、物体识别、社会互动和空间导航等复杂任务。

此外，人类还自然具备对时间变化的感知能力，能够无缝整合运动感知与时间意识，这对于协调动作和做出决策至关重要 \[549]。而自然界中的动物则展现出更为多样的感知能力。例如，鸟类和某些海洋生物能通过磁感知（magnetoreception）利用地球磁场进行导航，而鲨鱼和电鳗则利用电感知（electroreception）感应其他生物释放的电信号——这些都是人类所不具备的能力 \[550]。

与生物感知不同，人工智能智能体依赖于工程化的传感器，将环境刺激转换为可供算法处理的数字信号。常见的AI智能体传感器包括视觉传感器（摄像头）、听觉传感器（麦克风）、触觉传感器以及惯性测量单元（IMU）。AI智能体在处理视觉、听觉和文本数据方面通常表现出色，这得益于深度学习和信号处理技术的进步。

然而，一些人类特有的感知能力——尤其是味觉和嗅觉——仍然难以被机器准确模拟。例如，研究人员开发的先进仿生嗅觉芯片目前最多只能区分约24种气味，而人类的嗅觉系统则能够区分超过4000种不同的气味 \[551, 552]。


另一个关键区别在于感知处理的效率。人类的感知受限于生物学条件，例如神经传导速度通常在毫秒级。而人工智能系统处理感知输入的速度可以达到微秒甚至纳秒级，主要受限于计算硬件的性能，而非生物机制的限制。尽管如此，人类能够自然地将来自多种感官的输入整合为连贯的体验，这种能力被称为**多模态感知（multimodal perception）**，且几乎不需要额外努力。而对于AI智能体而言，实现这种多模态整合则需要精心设计的融合算法，以显式地将来自不同传感器的数据融合，构建统一的环境表征 \[553]。

在人类与人工智能智能体处理**时间与空间信息**的方式上也存在进一步的差异。人类的感知本质上是连续和流动的，能够顺畅地感知时间流逝与空间运动，而无需明确的时间离散化。相比之下，AI智能体通常依赖对传感器数据的离散采样，并通过时间戳或顺序处理来模拟这种连续性。人类的空间感知能自然地融合视觉、听觉与前庭信息，从而实现直觉式的空间定位。而人工智能的空间感知则往往依赖于算法手段，比如\*\*同步定位与建图（SLAM）\*\*或基于视觉数据序列的三维场景重建 \[554]。

外部环境中的物理或化学刺激被传递至人类的感官器官（如眼睛、耳朵、皮肤等），并由感知系统接收后转化为神经信号，最终由大脑处理，产生对环境的感知。同样地，为了使智能体能够与环境连接，获取这些感知内容也是至关重要的。目前，主要通过各种传感器将电信号转换为可处理的数字信号，以供感知系统使用。

在本节中，我们将根据输入中涉及的模态数量及是否进行统一融合建模操作，将模型分为**单模态模型（Unimodal Models）**、**跨模态模型（Cross-modal Models）**和**多模态模型（Multimodal Models）**。

* **单模态模型**专门处理和分析来自单一模态或输入类型（如文本、图像或音频）中的数据；
* **跨模态模型**通过特定的映射机制建立不同模态之间的关系，并实现模态间的转换；
* **多模态模型**则整体性地同时整合和处理多种模态，以利用其互补信息，从而实现更全面的理解与决策。

## **7.2 感知表示的类型**
### **7.2.1 单模态模型**

当人类置身于环境中时，可以聆听美妙的音乐，欣赏日出与日落，或是在舞台上体验一场精彩的视听盛宴。这些感知内容既可以是单一的图像或音频，也可以是多种感知内容的融合。就智能体的感知输入类型而言，我们将从**单模态**和**多模态**输入开始介绍它们的实现方式和差异。

---

#### 文本（Text）

作为重要的交流媒介，文本承载着丰富的信息、思想、情感和文化。人类通过视觉、听觉或触觉间接获取文本内容，这是人类与环境交互的重要方式之一。而对于智能体而言，文本可以直接作为与环境连接的桥梁——以文本为输入，输出响应内容。除了字面意义，文本还包含了丰富的语义信息和情绪色彩。
早期常用**词袋模型（Bag-of-Words）**\[555] 来统计文本内容，广泛应用于文本分类场景，但无法获取语义表达。BERT \[485] 使用**双向 Transformer 架构**进行语言建模，通过大规模无监督预训练捕捉文本的深层语义信息。\[486, 487] 则进一步优化了 BERT 的训练效率。以 GPT3.5 \[556] 为代表的自回归模型开启了\*\*大语言模型（LLM）\*\*的时代，进一步统一了文本理解与文本生成任务，而像 LoRA \[109] 等技术显著降低了 LLM 的应用成本，提升了智能体在复杂真实场景任务中的感知能力。

---

#### 图像（Image）

图像是人类与环境交互的另一重要方式，其天然地编码了空间信息，包含了物体的形态特征、空间位置、维度关系和运动属性等关键属性。计算机视觉架构的发展极大推动了对这些空间特征的处理能力。
经典的 ResNet 架构 \[488] 奠定了深度视觉特征提取的基础原则，后续的 YOLO 系列 \[557, 558] 展示了在**高效地同时完成目标定位与分类**方面的能力。DETR \[489] 的提出带来了范式转变，通过**全局上下文推理实现并行预测**，有效消除了传统目标检测中非极大值抑制和锚点生成等计算开销。近期的 DINO 1.5 \[490] 通过架构创新、骨干网络增强与训练策略扩展，将检测能力拓展到开放集场景，显著提升了人工智能体在非受限环境中的感知泛化能力。

---

#### 视频（Video）

视频是连续图像帧的表达形式，引入了时间维度，并通过图像帧的连续变化展现动态信息。智能体以视频为输入，可以通过连续帧获取更丰富的感知内容。
ViViT \[491] 能从视频中提取时空标记，有效分解输入的时间与空间维度；VideoMAE \[492] 通过自监督预训练学习通用视频特征表征，在异域数据上表现出强大的泛化能力，为智能体在新场景中获取感知能力奠定了基础。

---

#### 音频（Audio）

除了文本和视觉，人类与环境交互的另一重要方式是音频。音频不仅包含直接的文本内容，还包含了说话者的语调与情感信息 \[559]。
Wav2Vec2 \[495] 通过量化潜在表示来定义对比任务，实现了在仅使用 1/100 标注数据量的情况下达到较好的语音识别效果；FastSpeech 2 \[493] 直接引入了语音变化信息（如音高、能量、时长等），并通过真实目标进行训练，实现了更自然的文本转语音转换；Seamless \[494] 则通过流式生成与高效的单调多头注意机制，实现了低延迟的目标语言翻译，同时保持人声风格，可支持从多种源语言到目标语言的语音到语音/文本同步翻译。借助这些手段，智能体具备了“听”和“说”的能力。

---

#### 其他感知（Others）

目前，大多数关于智能体的研究集中于上述几种常见的感知输入类型。然而，就像人类拥有超过 20 种感知方式一样，智能体在其他感知能力方面也取得了一定进展。

例如：

* **香港科技大学**开发的仿生嗅觉芯片 \[551]，在纳米孔基底上集成了碳纳米管传感器阵列，每个芯片上可拥有多达 10,000 个独立寻址的气体传感器，类似于人类及其他动物的嗅觉系统，能精准区分混合气体和 24 种不同气味；
* **同济大学**\[560] 结合荧光与磷光信号，研发出具有多模光响应的智能味觉传感器，能有效识别鲜味、酸味与苦味；
* 为实现类人感知与抓取能力，**纽约大学**推出了一种低成本的磁性触觉传感器 AnySkin，可快速组装与更换；
* 甚至在“疼痛”感知方面，**中国科学院**借助液态金属粒子膜在“受伤”（如机械划伤）时表现出的独特电特性，实现了对“伤口”的感知与定位。

此外，部分工作如 HuggingGPT \[152]、LLaVA-Plus \[500]、ViperGPT \[498] 等，将上述各种单模态感知能力集成于框架中，根据任务需求进行选择与应用，从而实现更复杂的任务目标。



---

### 7.2.2 跨模态模型

**文本-图像跨模态**
近年来，整合文本和图像的跨模态模型取得了显著进展，提升了这两种模态之间的对齐、检索和生成能力。这类模型主要可以按目标分为：跨模态对齐与检索、文本生成图像、图像生成文本。

跨模态研究的一个核心方向是文本与图像的对齐与检索。OpenAI 在 2021 年提出的 CLIP \[51] 使用对比学习对文本与视觉表征进行对齐，使得模型具备零样本的跨模态检索和分类能力。同年，Google 推出的 ALIGN \[501] 利用大规模噪声网页数据优化文本与图像的嵌入对齐能力。2022 年，CyCLIP \[562] 引入循环一致性损失（cyclic consistency loss），进一步增强了跨模态对齐的鲁棒性，提高了检索任务的可靠性。

另一个关键方向是文本生成图像，目标是根据文本描述合成高质量图像。OpenAI 的 DALL·E 系列 \[563, 564, 502] 从 2021 年至 2023 年在该领域做出了重要贡献，DALL·E 3 能够实现对生成图像的细粒度语义控制。Stability AI 于 2022 年推出的 Stable Diffusion \[565] 采用基于扩散的生成方法，支持开放领域的文本生成图像以及跨模态编辑。

第三个重要方向是图像生成文本，即基于图像输入生成高质量的文本描述。典型代表是 Salesforce 于 2022 到 2023 年间提出的 BLIP \[566] 与 BLIP-2 \[567] 模型，它们通过轻量级桥接模块加强了视觉-语言模型的集成，支持图像字幕生成、问答等任务。

**文本-视频跨模态**
该领域主要研究视频文本对齐、生成与检索。VideoCLIP \[504] 使用基于时序卷积或 Transformer 的视频编码器从视频帧中提取时序特征，再与语言编码器生成的文本表征进行对齐，从而实现稳健的视频-文本关联。在文本生成视频方面，Meta 提出的 Make-A-Video \[506] 利用扩散技术扩展空间-时间维度，实现了从文本到高质量视频的合成。Google 的 Phenaki \[505] 聚焦生成长时间、时间连贯的视频序列，在跨模态学习方面表现出显著进展。DeepMind 的 Frozen in Time \[568] 采用对比学习用于视频文本匹配，有效支持跨模态检索任务，提升了基于文本的相关视频片段搜索能力，加强了视觉与语言的融合理解。

**文本-音频跨模态**
连接文本与音频的跨模态模型在模态表征、生成与转换等相关任务中取得了显著进步，也提升了单一模态下的感知能力。2021 年提出的 AudioCLIP \[509] 将 CLIP 框架扩展到音频领域，使得图像、文本与音频之间实现三模态检索。AudioCLIP 采用多任务学习方式，将三种模态表征统一到共享嵌入空间，提升了跨模态检索与交互的能力。类似地，VATT \[508] 使用统一的 Transformer 架构，分别对视频、音频与文本进行独立编码，再将其融合至共享的多模态空间，支持跨模态检索与多任务学习，适应更多样的多模态应用场景。

在文本生成音频方面，Meta 于 2023 年推出的 AudioGen \[569] 实现了从文本描述直接合成环境音效与音乐片段等音频内容。这展示了 AI 在根据语言输入生成高保真音频方面日益增强的能力，为媒体、娱乐与辅助工具等领域拓展了应用空间。

此外，在语音转文本（Speech-to-Text）与文本转语音（Text-to-Speech）转换方面，微软推出了 SpeechT5 \[570]。该模型将语音与文本的生成统一于一个框架，支持语音识别与合成的双重功能。通过共享架构的设计，SpeechT5 实现了语音与文本处理的无缝集成，增强了自动转录、语音助手及无障碍工具等场景下的应用效果。

**其他**
在其他特定场景与领域中，跨模态建模同样发挥着重要作用。CLIP-Forge \[510] 提出了一种从文本描述生成三维形状的新方法，利用 CLIP 的语言-图像对比预训练能力，实现了根据自然语言输入合成高质量 3D 对象，从而连接文本与三维几何之间的桥梁。Point-E \[511] 在此基础上进一步实现了从文本生成 3D 点云，与传统的三维重建方法不同，Point-E 聚焦于点云表示，使得三维内容生成更高效、可扩展，同时保持对文本提示的高忠实度。

在医学影像领域，MoCoCLIP \[571] 提出一种增强零样本学习能力的方法。该方法将 CLIP 与动量对比学习（MoCo）相结合，提升了深度学习模型在医学影像应用中的泛化能力，有效应对标注数据稀缺与领域适应等挑战。

---

以下是你提供的英文内容的中文翻译：

---

### 7.2.3 多模态模型

上述跨模态模型主要通过对比学习等方式，在不同模态之间进行对齐和映射，以实现模态间的信息互补与转换。而多模态模型的研究则进一步聚焦于如何融合多种数据（如视觉、文本、音频等）的特征，以提升整体模型的性能。

#### 视觉语言模型（VLM）

视觉语言模型（Vision Language Model，VLM）广义上指的是能够同时从图像（或视频）和文本中学习的多模态模型。人类生活在一个充满多模态信息的世界中，视觉信息（如图像和视频）与语言信息（如文本）往往需要结合在一起才能完整表达含义，智能体也需要类似的能力。

LLaVA \[513] 首次尝试使用 GPT-4 生成多模态语言-图像指令数据集，通过端到端训练获得了一个大型多模态模型，并展示了卓越的多模态对话能力。LLaVA-NeXT \[513] 采用动态高分辨率和混合数据，在纯英文模态数据上也展现出惊人的零样本能力，且计算/训练数据成本比其他方法低 100\~1000 倍。

Emu2 \[516] 改变了传统使用图像 tokenizer 将图像转换为离散 token 的方式，直接使用图像编码器将图像转换为连续嵌入向量并输入 Transformer，从而增强多模态上下文学习能力。MiniGPT-v2 \[512] 在训练过程中为不同任务设计了唯一标识符，有助于模型更有效地区分任务指令，提高了各项任务的学习效率。

Qwen2-VL \[515]、DeepSeek-VL2 \[572] 对视觉部分采用动态编码策略，旨在处理不同分辨率的图像，生成更高效、准确的视觉表示。同时，DeepSeek-VL2 \[572] 还引入了具备多头潜在注意力机制的 MoE 模型，将键值缓存压缩为潜在向量，实现高效推理。

早期工作多以图文融合训练为主。Video-ChatGPT \[573] 将输入扩展为视频，直接使用视频自适应视觉编码器结合大语言模型进行训练，捕捉视频数据中的时序动态和帧间一致性关系，从而实现对视频内容的连贯开放式对话。为了解决图像和视频缺乏统一 token 表达的问题，Video-LLaVA \[574] 将图像和视频编码的视觉表示统一到语言特征空间，使两者互相增强。

类似地，Chat-UniVi \[575] 使用一组动态视觉 token 来整合图像和视频，并采用多尺度表示，使模型能同时掌握高层语义概念与低层视觉细节。优酷的 Youku-mPLUG \[576] 针对特定场景进行了深入研究，基于优酷视频平台上的高质量中视频-文本对，增强了模型理解整体和细节视觉语义及场景文字识别的能力。

不同于需要训练的方法，SlowFast-LLaVA \[577] 通过双流 SlowFast 架构，无需对视频数据进行额外微调，也能有效捕捉视频中的空间细节语义和长期时序上下文，达到甚至优于微调方法的效果。

随着大模型参数逐步缩小、端侧算力持续增强，高性能的端侧模型正成为趋势。手机、PC 等智能终端对图像视觉处理的需求强烈，对部署在端侧的 AI 模型提出更高的多模态识别效果与推理性能要求。TinyGPT-V \[517] 基于 Phi-2 \[578] 小型主干网络结合 BLIP-2 \[567] 构建，仅需 8G 显存或 CPU 即可推理，有效解决 LLaVA \[513] 与 MiniGPT-4 \[579] 的计算效率问题。

MiniCPM-V \[519] 主要为长图与复杂图像提供强大的 OCR 能力，且产生幻觉率低，输出更可靠。Megrez-3B-Omni \[580] 通过软硬件协同优化，确保所有结构参数高度兼容主流硬件，其推理速度较同等精度模型快 300%，显著提高了对不同终端硬件的适配能力。

同样，越来越多与 GUI 相关的研究关注在手机和 PC 上自动执行任务。OmniParser \[520] 使用网页和图标描述数据集进行微调，显著提升了对截图中图标的检测与功能语义表达能力。GUICourse \[581] 和 OS-ATLAS \[582] 构建了跨平台 GUI 锚定语料库，在 GUI 截图理解与交互知识丰富度方面取得显著进展。

#### 视觉语言动作模型（VLA）

视觉-语言-动作（VLA）模型以视觉与语言为输入，生成机器人动作为输出，是具身智能领域的重要研究方向。VLA 模型的视觉与语言编码器选择经历了从早期 CNN 到 Transformer 架构的多样演变，进一步集成了 3D 视觉与大型语言模型。

早期模型如 CLIPort \[521] 使用 ResNet \[488] 处理视觉输入，结合语言嵌入生成动作，奠定了多模态融合的基础。RT-1 \[522] 引入 Transformer 架构，采用 EfficientNet 作为视觉编码器、USE 作为语言编码器，通过 FiLM 机制融合视觉与语言信息，显著增强了模型的泛化能力。

VIMA \[523] 进一步采用多模态提示，结合 ViT 视觉编码器与 T5 语言模型，以支持更复杂的任务。PerAct \[524] 创新性地使用 3D 点云作为视觉输入，并通过 Perceiver IO 处理多视角信息，为机器人操作提供更丰富的空间感知。

Diffusion Policy \[525] 将 ResNet 与 Transformer 结合，通过扩散模型生成动作，提升动作生成的多样性与准确性。SayCan \[583] 将 PaLM 语言模型与视觉输入结合，使用 CLIP 编码器进行任务分解。PaLM-E \[526] 结合 ViT 与 PaLM，通过文本规划引导低层级动作执行。MultiPLY \[527] 更进一步将 3D 信息融入 LLM，结合 EVA 与 LLaMA 提供更全面的复杂任务规划能力。

#### 音频语言模型（ALM）

音频语言模型（Audio Language Model，ALM）使用音频与文本构建多模态模型。SpeechGPT \[533] 构建了大规模跨模态语音指令数据集 SpeechInstruct，并训练了离散语音表示，实现了超预期的语音跨模态对话能力。

LauraGPT \[584] 不同于传统通过离散 token 表示音频的方式，提出结合音频连续和离散特征的新型数据表示方法，并通过监督式多任务学习在多个音频任务中展现出优异表现。\[529, 585, 531] 将音频数据转为嵌入表示，并通过指令微调，在多个语音处理任务中通过自然语言指令取得良好效果。

为降低微调成本，Audio Flamingo \[528] 通过上下文学习与检索增强未见任务的适应能力。UniAudio 1.5 \[530] 使用文本中的词或子词作为音频 token，在少量样本下学习这些音频表示，实现无微调的跨模态输出。为提升输出的真实感和人类偏好对齐，Qwen2-Audio \[54] 引入 DPO 训练方法，实现与人类偏好的对齐。

#### 音频视觉语言模型（AVLM）

音频视觉语言模型（Audio Vision Language Model，AVLM）融合音频、视觉和文本，统一多模态模型。前文已介绍一些使用双模态信息构建多模态模型的工作。在追求通用人工智能（AGI）的过程中，任务与模态的多样性和异质性是关键障碍。

一种可行策略是在统一框架下支持更多模态能力。一些闭源工作 \[586, 587] 已在文本、视觉与音频等模态上展现出卓越能力。ImageBind \[588] 实现了图像、文本、音频、深度、热成像和 IMU 数据等六模态联合嵌入。

Panda-GPT \[535] 结合 ImageBind 多模态编码器与 Vicuna \[589]，在图文之外展现出零样本跨模态能力。类似的还有 \[539, 536]，通过视觉、音频与文本编码信息对齐和训练实现跨模态能力。

由于多模态模型通常训练成本高，UniVAL \[538] 基于任务平衡与多模态课程学习，仅使用约 2.5 亿参数进行训练，并通过权重插值合并多模态模型，实现了良好的泛化能力。

NExT-GPT \[542] 将大语言模型与多模态适配器和不同扩散解码器连接，仅训练少量投影层（1%）的参数。

其他工作如 \[543, 590, 544, 545] 实现了任意模态间的输入输出转换。Unified-IO 2 \[543] 是首个可理解与生成图像、文本、音频和动作的自回归多模态模型，通过统一的语义空间 token 表达不同模态输入，并使用编码器-解码器处理。

AnyGPT \[590] 构建了首个大规模任意模态指令数据集，统一使用离散表示处理各种模态输入。Modaverse \[545] 直接将 LLM 的输出与生成模型的输入对齐，避免了传统方法对非文本与文本隐空间对齐的复杂依赖。

CoDi-2 \[544] 在基于主题的图像生成、视觉转换和音频编辑任务中表现超越了以往的领域特定模型。

#### 其他

相比于 3D 世界，人类对 2D 世界的探索更为深入，但 3D 能更准确描述物体形状和纹理，提供更丰富的感知信息。PointLLM \[540] 使用点云编码器表示几何与外观特征，并融合语言特征进行两阶段训练，实现出色的 3D 物体描述与分类能力。

由于 3D 信息比 2D 更丰富，训练成本也更高。\[541, 591] 试图降低训练开销。MiniGPT-3D \[541] 使用 2D-LLM 的先验对齐 3D 点云与 LLM，通过级联方式进行模态对齐，并混合查询专家模块，有效汇聚特征，实现小参数更新下的高效训练。

LLaVA-3D \[591] 将 2D CLIP patch 特征与其在 3D 空间中的位置对应，整合 3D patch 到 2D 多模态模型中，通过联合 2D 和 3D 语言指令调整，实现训练收敛速度提升 3.5 倍。

为使智能体能准确感知和操作未知物体，Meta \[592] 开发了 NeuralFeels 技术，结合视觉与触觉，在 3D 中持续建模未知物体，在手持操作中更准确估计物体姿态与形状，提升了对未知物体操作的准确率高达 94%。

---

以下是该段英文的中文翻译：

---

### 7.3 感知系统优化

感知错误（包括不准确、误解和“幻觉”（生成虚假信息））给基于大语言模型（LLM）的智能体的可靠性和有效性带来了重大挑战。因此，优化感知系统需要在模型层、系统层和外部层面采取多种策略来尽量减少这些错误。

#### 7.3.1 模型层优化

**微调（Fine-tuning）**：在特定领域数据上微调预训练的LLM能显著提升其准确感知和解释相关信息的能力。例如，在城市导航任务中，将LLaVA等模型在特定地标上微调，被证明能提升识别准确性 \[513, 593]。此外，低秩适配（Low-Rank Adaptation，LoRA）等技术可以更高效地进行微调，在不显著增加模型复杂度的前提下提升性能 \[109, 594]。一些LLM与传统视觉模型的结合也被广泛应用，例如基于Llama-Adapter \[596]架构集成YOLOS \[595]，显著提高了检测与定位能力。

**提示工程（Prompt Engineering）**：设计有效的提示语对生成既准确又符合预期目标的输出至关重要。通过提供清晰指令、上下文信息和特定格式要求，提示工程可以最大程度地减少误解和幻觉 \[597]。系统提示定义了智能体的角色，历史提示提供了上下文，定制提示确保输出一致性，这些方式已被证实能显著减少错误 \[597]。

**检索增强生成（Retrieval-Augmented Generation）**：通过检索机制为LLM提供外部知识来源，有助于其生成更有事实依据的回答，从而减少幻觉现象并提升感知信息的准确性 \[334]。

#### 7.3.2 系统层优化

**预期-再评估机制（Anticipation-Reevaluation Mechanism）**：在面对不完整或模糊信息的场景中，采用预期-再评估机制可以增强鲁棒性。例如，在导航任务中，智能体可以基于历史数据预判目标方向，并在获取新信息后对推断结果进行再评估 \[598]。

**多智能体协作（Multi-Agent Collaboration）**：在多智能体系统中，结构化的沟通与协作有助于信息共享、错误修正和达成共识，从而更准确地感知环境 \[599]。不同的通信拓扑结构（如全连接、集中式和分层结构）在效率与鲁棒性之间提供了不同的权衡 \[600]。InsightSee \[601]通过一个具备描述、推理与决策功能的多智能体框架来优化视觉信息，有效提升视觉处理能力。同样，HEV \[602]将多个智能体的全局视角信息整合，通过协同感知赋予强化学习智能体全局推理能力，增强其决策能力。

**智能体专业化（Agent Specialization）**：在多智能体系统中为每个智能体分配特定角色和能力，可以实现感知任务的分工协作，使每个智能体专注于环境或任务的某一方面，从而提升整体感知的准确性和效率 \[603]。

#### 7.3.3 外部反馈与控制

**用于优化的损失智能体（Loss Agents for Optimization）**：将LLM作为损失智能体，可以在训练过程中动态调整损失函数的权重 \[604]，从而基于复杂甚至不可微分的目标（如人类反馈或专业模型评价）来优化图像处理模型。这种方法本质上是将优化目标外部化，让LLM能够“感知”并适应复杂标准 \[605]。

**人类在环系统（Human-in-the-Loop Systems）**：引入人类反馈和监督有助于纠正错误、引导智能体学习过程，并确保其行为符合人类价值观和期望 \[43]。

**内容与输出中介（Content and Output Mediation）**：在将LLM的输出展示给用户前，进行内容中介处理以过滤和优化这些输出。这有助于防止意外或有害行为，确保输出符合用户期望和安全指南 \[606]。

---

**7.4 感知应用**

智能体的运行效率主要受三个关键因素的影响：模型结构的维度、硬件基础设施的规格，以及量化优化方法。随着模型参数的指数级增长——从 BERT-Base 的 1.1 亿参数，到 GPT-3 的 1750 亿，再到 LLaMA 3 的前所未有的 4050 亿参数——处理延迟也随之从毫秒级增长到数百毫秒。硬件性能差异尤为显著：以 GPT-3 为例，NVIDIA H100 相比 A100 在处理 token 时吞吐量提高了 50%，而 RTX 4090 的处理能力约为 A100 的两倍。

当前智能体已广泛渗透到各类领域，包括个人助手系统、游戏环境、机器人流程自动化（RPA）和多媒体内容生成，视觉感知仍是最主要的输入模态。在像 Minecraft 这样的程序生成环境中，STEVE \[607] 通过视觉信息处理，实现了技术树推进速度提升 1.5 倍、方块搜索效率提升 2.5 倍的显著性能提升。Steve-Eye \[608] 进一步在此基础上进行端到端多模态训练，通过整合视觉与文本输入，缓解了环境理解的延迟问题。

在创意内容生成方面，AssistEditor \[609] 通过多智能体协作，实现了风格驱动的内容理解，支持专业视频编辑。类似地，Audio-Agent \[610] 实现了文本/视觉输入与音频输出的跨模态集成，具备全面的音频处理能力 \[611, 612, 613]。

在移动和桌面平台，智能体应用也取得了显著进展。例如，ExACT \[614] 在 VisualWebArena \[615] 中通过基于截图的探索学习与掩码集融合，达成了 33.7% 的成功率，创下了新的基准成绩。SPA-Bench \[616] 提出了一个真实反映现实世界复杂性的全面移动端评估框架。M3A \[617] 通过多模态输入处理，在 SPA-Bench 中表现出色，达成了 64.0% 的成功率。AgentStore \[618] 则通过改进视觉与可访问性树处理能力，将 OSWorld PC 基准表现提升到了 23.85%。

在个人 AI 助手方面，语音交互能力 \[619, 586] 显著降低了交互摩擦，并提升了操作效率。其中，语音情感韵律的引入有效增强了用户参与度与留存率。

在具身智能应用中，触觉与力反馈机制成为环境交互的重要模态。增强的感知精度正在推动操作能力向更高精度发展 \[620]。

---

**7.5 总结与讨论**

尽管越来越多的研究 \[543, 590] 致力于构建支持多感知能力输入输出的统一多模态模型，但作为自主系统基石的智能体感知，在多模态数据的有效解释与融合方面仍面临重大挑战。目前的方法在表示学习、对齐与融合等方面存在持续性问题，这阻碍了稳健且可泛化的感知系统的发展。

其中一个主要问题在于表示方法本身，往往难以捕捉多模态数据中复杂微妙的语义特征。尤其在高维感知输入的情境下，现有方法难以进行保留语义信息的抽象。此外，多模态表示之间的对齐也充满挑战，将异质数据整合为统一特征空间不仅计算代价高昂，而且容易产生不一致，进而导致对模糊信号的误解。这一问题在特征融合阶段更为严重，不同模态的特征整合往往效果不佳，甚至丢失关键信息。

未来的研究方向应优先考虑通过**动态神经结构**实现**自适应表示学习**，可根据环境上下文和任务需求自动调整结构，这包括基于元学习参数化或图结构的表示方法，用以明确建模感知实体间的关系。针对跨模态对齐，结合对比学习原理的**自监督时空同步机制**展现出潜力，能够在不依赖大量标注数据的前提下建立稠密对应关系。将**因果推理框架**引入对齐流程中 \[621]，有望增强系统对伪相关的鲁棒性。

在表示融合方面，**分层注意力机制**与**可学习门控函数**的结合值得深入研究，可实现对上下文敏感的模态特征整合。新兴的**可微分记忆网络**技术也可能为跨时间范围内的特征维护与更新提供新路径。



