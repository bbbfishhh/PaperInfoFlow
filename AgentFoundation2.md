# Part 1 Core Components of Intelligent Agents

人脑的三大能力：学习、推理、规划

1. 人类学习通常伴随着主动的**好奇心、动机以及情感强化**，而基于LLM的智能体通常是通过更形式化的过程进行学习，比如训练期间的参数更新，或探索过程中形成结构化的记忆。

2. 反复出现的“空间”、“目标”

如图所示，也是学习的两大分类。

【我的理解】：人的学习分为 空间 和 目标。

空间是指人学习 在大脑中的层次，是属于什么空间层次的学习，是整体空间的学习：比如经历了一次重大变故之后，世界观的重塑；比如顿悟，就是说的整体空间的智慧重塑。

局部空间，则是指一个小块区域的学习。比如不同的职业有不同的局部学习空间，运动员是肌肉，音乐是手指和乐理，工程师是工程思维和上手操作，是属于视觉、触觉、听觉等的局部控制。

LLM 类比则是，全局的预训练，无监督的空间整体学习。 微调、强化学习则是局部空间的学习。

目标，则是人学习的目标。

是提升感知的目标，还是提升推理能力的目标，还是发展世界理解能力的目标。

不同的人有不同的空间长板，人应该发现自己能力圈的边界，即空间长板的。对应的目标也应该是空间长板对应的目标。


---

# 2.1 学习

## 学习空间
人类学习依靠大脑中可塑的神经网络，在多个空间和目标上并行进行。

大脑通过整合系统在整个神经网络中协调学习：海马体促进情境经验的快速编码，小脑支持精细运动技能的监督学习，基底神经节通过多巴胺奖励信号实现强化学习，而大脑皮层区域则执行无监督的模式提取。

### 完全心理状态学习（Full Mental State Learning）

对底层模型的修改可被视为完全心理状态学习，因为它会从根本上改变智能体的能力，并影响所有心理状态组件。

对应大模型：

后训练技术（Post-training techniques） 是提升智能体能力的关键，类似于人类大脑通过教育被塑造的过程。

各种微调（Tuning）方法使智能体能够获得特定领域的知识和逻辑推理能力：
- SFT（监督微调）：从人工标注示例中学习，将知识直接编码进模型权重。
- PEFT（参数高效微调）：如 Adapter-BERT 采用模块化设计，仅对部分参数进行更新；LoRA 将权重更新分解为低秩矩阵，只调整小部分有效参数。
- RLHF（基于人类反馈的强化学习）：通过训练奖励模型和策略优化，使模型与人类价值一致（如 InstructGPT）。
- DPO（直接偏好优化）：在无需显式奖励建模的情况下直接进行偏好学习，保留对齐效果同时降低计算复杂度。

强化学习（RL） 在特定环境中提供了有前景的专用学习路径，特别有助于提升推理能力，使模型能够在“思维空间”中学习：

- ReFT：使用自动采样的推理路径和在线强化学习奖励，提升推理能力。

- DeepSeek-R1：引入基于规则的奖励和 GRPO（群体相对策略优化）方法。

- Kimi k1.5：结合上下文强化学习与优化的思维链技术，提高规划与推理效率。

- DigiRL：通过两阶段强化学习，使智能体能在 Android 设备模拟器上执行多种命令。


### 部分心理状态学习（Partial Mental State Learning）

CoT（思维链） 是这种方法的代表，它展示了模型如何在保持基础参数不变的同时增强推理能力。

- Generative Agents：通过日常交流与社交互动积累并重放记忆，从中提取高阶信息，用于动态行为规划。
- Voyager：在 Minecraft 环境中持续更新技能库，积累过程知识，无需重新训练模型。
- Learn-by-Interact：通过环境交互合成经验数据，省去了人工标注与复杂的强化学习框架。
- Reflexion：通过反思和文本反馈机制，从错误中学习并改进行为。


## 学习目标

### 感知能力的学习

有效感知和处理来自环境的信息是智能体智能的基础。为增强感知能力，智能体主要采用两种学习路径：扩展多模态感知和利用检索机制。

CLIP 实现了视觉与语言表示在共享嵌入空间中的对齐。在此基础上，LLaVA 通过图文对训练专用投影器提升视觉感知，而 CogVLM 则通过统一表示架构推动了视觉推理能力的发展。

感知模态的拓展也延伸到了多个感官领域。在音频处理方面，QwenAudio 展示了从语音到环境声的多种声音信息的统一编码能力。

此外，智能体通过检索机制提升观察能力。与人类依赖即时感官输入不同，智能体可以学习从庞大的外部知识库中访问并整合信息。检索增强方法如 RAG 通过将当前观察与相关知识相连接，增强了感知理解。

检索驱动的智能体进一步展现了增强主动信息获取能力的潜力。例如 Search-o1 通过提示引导推理模型学习主动检索；R1-Searcher 和 Search-R1 则将检索能力直接嵌入模型中，使模型能够在推理过程中自主检索信息。这些进展表明，通过提升模型层面的主动感知能力，是未来智能体发展的关键方向。


### 推理能力的学习

推理是连接智能体心理状态与行为的桥梁。

现代智能体推理能力的基础主要包括：一是其模型中蕴含的丰富世界知识，二是模型内部或上下文结构中所支持的逻辑框架。

推理能力的发展呈现出以下趋势：

1. 高质量推理数据显著增强模型的推理能力；

2. 此类数据的获取通常依赖验证机制或奖励模型；

3. 直接在基础模型上进行强化学习训练也能自发激发推理能力。

另一重要观察是：结构化的高质量推理数据更能促使模型掌握推理过程。

一种可行的探索路径是：先进行广泛推理搜索，再借助验证环境或奖励模型对推理轨迹提供反馈，筛选出高质量数据。由此催生出多类强化推理机制。

第一类是以 STaR 为代表的自举范式（bootstrap paradigm），模型生成推理路径后进行自我迭代优化。例如 Quiet-STaR、V-STaR、rStar-Math 等，后者结合强化学习提升数学推理能力。

第二类是更明确引入强化学习原理的 ReST 系列。原始的 ReST 模型提出“强化自训练”（Reinforced Self-Training），每个样本尝试多次，仅保留成功推理路径。ReST-EM 引入期望最大化，ReST-MCTS 则结合蒙特卡洛树搜索实现更复杂的推理策略。

在此基础上，强化学习（RL）在提升语言模型推理能力方面也取得显著成效。例如 DeepSeek R1 和 Kimi-K-1.5 就是基于 RL 框架开发的。ReFT 首创将监督微调与在线 RL 结合，VeRL 提供支持多种 RL 算法的开源框架，RFT 则在具体推理任务中验证了奖励引导优化的效果。

后续研究进一步将 RL 拓展到通用推理与特定场景，如 OpenR1、RAGEN 推广至一般推理，SWE-Gym 成功应用于软件工程任务，DigiRL 专注于数字世界智能体增强。

近期工作更将工具调用融入 RL 推理流程，例如 Qwen-QwQ-32B 利用奖励机制整合工具调用，RAGEN 聚焦于多步骤任务，构建智能体强化学习框架。这些进展表明模型训练与智能体开发的融合趋势正在加速。

### 对世界的理解能力学习

是能通过直接交互和经验积累来理解世界的运作规律。

在基础层面，Inner Monologue 展示了智能体如何通过持续交互积累环境知识；Learn-by-Interact 则证明无需显式奖励也能从互动中获得有效理解。更复杂的方法包括 Minecraft 场景中的 DESP 和 Voyager，前者通过结果分析进行经验处理，后者通过动态技能库扩展优化学习能力。


经验的处理与利用已被系统化。Generative Agents 引入高级记忆重放机制，从过去交互中提取高层次洞察；Self-refine 与 Critic 则构建了系统化的经验评估与优化循环。


奖励理解的优化也成为世界理解的关键组成。Text2Reward 展示了通过人类反馈持续优化奖励函数，提升其与任务目标和环境特征的契合度。AutoManual 则通过持续互动构建行为指南，从而发展出奖励验证机制，为决策提供依据。



在这些基础上，RAP 提出了将推理视作基于世界模型的规划方式。RAP 将大模型既作为推理代理又作为世界模型，通过蒙特卡洛树搜索模拟可能动作结果，从而实现更有效的规划与探索。


进一步创新如 ActRe 反转传统的“推理-行动”序列，先执行再生成解释，从而体现出语言模型对世界动态的内在理解，支持轨迹标注与对比式自训练。

# 2.2 推理 reasoning

推理是实现智能行为的关键，它将原始信息转化为可操作的知识，从而推动问题解决和决策制定。

在人类认知中，推理通过多种策略体现出来：演绎推理将一般规律应用于具体情形，归纳推理从个别实例中构建出一般性结论，而溯因推理则在不完整信息中推导出最可能的解释。

这些过程通常会借助启发式方法——即在不确定性下简化决策过程的心理捷径——并通过环境反馈不断优化，使推理始终立足现实并具备适应变化的能力。

--丹尼尔 卡尔曼的三部曲

在基于LLM的智能体中，推理可以被正式定义为基于**心理状态（mental state）进行行动选择（action selection）**的过程，它在感知与行动之间架起了一座关键的桥梁。

推理动作的组合自然划分出两种不同的方式：结构化推理（structured reasoning）和非结构化推理（unstructured reasoning）。

结构化：即严谨推理

非结构化：即直觉，跳跃思维

### 线性顺序推理

ReAct 通过将推理轨迹与任务特定动作交替结合展示了这一点。这种组合允许推理轨迹引导并修改行动计划，而行动可以访问外部信息源以获取更多信息。

Reasoning via Planning（RAP）扩展了线性推理范式，将大模型（LLM）推理形式化为马尔可夫决策过程，尽管其受限于特定问题的状态设计。Markov Chain of Thought（MCoT）则进一步扩展了这一范式，将每一步推理概念化为伴随可执行代码的马尔可夫状态。该方法通过将先前的推理压缩为简化的数学问题，实现了无需长上下文窗口的高效下一步推理。

Atom of Thoughts 明确将问题定义为状态表示，并设计了通用的“分解-收缩”两阶段状态转移机制，以构建马尔可夫式推理流程，将复杂问题转化为一系列原子问题。

树状探索结构

Language Agent Tree Search（LATS）通过将蒙特卡洛树搜索（MCTS）与大模型结合

图结构推理


---

决策理论、心理学和控制论中的观点——例如理性框架、前景理论和反馈回路——说明了规划如何使人类超越被动反应行为，通过有意识的意图和自适应策略积极塑造未来。

LM 通常缺乏对世界动态的深层理解，更多依赖模式识别而非真正的因果推理，这限制了其处理子目标相互作用和环境变化的能力。

缺乏内在的“系统2”推理机制，也进一步限制了其独立生成结构化、最优规划的能力。为此，研究人员提出了任务分解、搜索优化与外部知识集成等策略以缓解这些问题。

