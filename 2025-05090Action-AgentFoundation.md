# Action

- 用 bento grid 画一个

1. 行动

行动系统的核心在于构建与环境的交互过程，并通过预定义的奖励函数从收集到的行为轨迹中进行学习。

如何在环境中建立行为学习机制；


2. 空间

你生命的活动空间有多大？

3. 行动序列 （优先级）

你人生的优先级是什么？

与机器相比，人类认知系统擅长通过现实世界的交互不断获取新知识，核心在于生成并优化行动序列。

4. Agent 行为系统设计三大要点：

- 如何从特定场景扩展到通用领域，为 AI 智能体构建行动空间；

- 如何在环境中建立行为学习机制；

- 如何利用外部状态（如工具）拓展 AI 智能体的任务边界。



在哲学领域中，行动被定义为智能体在环境中出于潜在或特定目的所能执行的行为。例如，操控、移动、推理和工具使用都可以被视为智能体在现实场景中为实现目标而执行的基本行动。换句话说，行动来源于智能体在环境中以目标为导向的参与过程，反映了其为实现目标而改变外部世界的意图。因此，行动系统在区分 AI 智能体与基础模型（如大型语言模型，LLMs）方面也起着至关重要的作用。

总体而言，现有的基础模型在多种任务上已经展现出令人印象深刻的性能，但其任务范围仍然受限，因为它们主要依赖于最初的预训练目标（例如，下一词预测）。当基础模型作为“大脑智能”使用时，配备了行动系统的 AI 智能体可以直接与环境交互，并执行复杂的用户意图。此外，行动系统还可以支持智能体利用外部环境中的现有工具，从而显著扩展其任务范围。

因此，行动系统的设计将决定 AI 智能体在感知、决策、执行、工具使用等各个方面是否能与人类大脑保持一致。换句话说，基础模型为智能体奠定了基础，而行动系统决定了其实现复杂目标的最终潜力。为 AI 智能体设计一个有效且全面的行动系统是一项关键任务，它不仅面临巨大挑战，也带来显著收益。

在图 8.1 中，我们展示了认知系统中行动系统的执行过程。在本节中，我们将首先在第 8.1 节讨论人类的行动系统，然后在第 8.2 节探讨从人类行动到 AI 智能体行动的转变。之后，在第 8.3 节中，我们将系统性总结现有 AI 智能体行动系统的不同范式，包括行动空间、行动学习和工具学习。在第 8.4 节，我们分析行动与感知之间的区别，最后在第 8.5 节中总结全文。

## **8.1 人类的行动系统**

人类认知中的行动系统是指使人能够感知、规划并执行以目标为导向的行为的一系列过程。它是一个复杂的系统，使个体能够与动态环境互动、做出决策，并根据反馈调整行为。通常来说，人类认知中的行动系统可以大致分为\*\*心理行动（mental action）**和**身体行动（physical action）\*\*两个类别：

* **心理行动**可以被视为一种独立的行动，它以思维过程的形式在大脑中驱动最终意图。例如，推理、决策、想象和规划等都可以被看作是不同类型的心理行动。换句话说，心理行动就等同于驱动人类身体行动以达成最终目标的大脑信号。

* **身体行动**是指由人体运动系统执行的任何以目标为导向的身体运动。在某种程度上，身体行动通常表现为一种连续的动作。例如，说话、操控、绘画、奔跑和抓握等都可被视为身体行动。通过一系列身体行动，人类可以与现实环境互动并收集反馈。

图 8.2 展示了一个从心理行动和身体行动两个维度出发的人类行动系统的简化分类图。人类的认知系统正是通过心理行动与身体行动的协同作用，才能在现实世界中应对多种复杂任务。

受到人类认知机制的启发，我们也有必要重新审视在 AI 智能体中，如何针对不同任务（从语言任务到数字环境再到物理环境）构建有效的行动系统。

---

表格 8.1 展示了不同类型基础模型的定义与目标：

| 模型类型          | 示例模型         | 输入类型  | 目标定义   | 简述                     |
| ------------- | ------------ | ----- | ------ | ---------------------- |
| 大型语言模型 (LLM)  | GPT-4 \[7]   | 语言输入  | 下一词预测  | 根据用户输入的提示生成文本。         |
| 大型多模态模型 (LMM) | LLaVA \[513] | 多模态输入 | 多模态生成  | 根据多模态输入生成对应的多模态数据。     |
| 机器人基础模型 (RFM) | RT-1 \[522]  | 感知输入  | 机器人控制  | 基于动态环境中的感知输入生成机器人控制指令。 |
| 大型行动模型 (LAM)  | LAM \[622]   | 环境交互  | 可执行的行动 | 基于与环境的互动生成可直接执行的行动。    |

---

[换图]

**图 8.2：人类行动的分类示意图（Illustrative Taxonomy of Human Actions）**

* **心理行动（Mental Actions）**

  * **认知类**：推理、规划、反思、想象、决策
  * **情感类**：情绪调节、动机驱动、共情
  * **记忆与学习**：记忆回忆、技能获取

* **身体行动（Physical Actions）**

  * **身体运动**：移动、手势、姿态调整
  * **物体使用**：操控、组装
  * **沟通行为**：口语与语言表达、书写与打字、非语言交流（如手语）

这些分类显示了人类行动在心理与身体两个层面上的多样性与复杂性。借助这样的认知模型，我们可以更深入地理解 AI 行动系统的设计目标与实现路径。

## **8.2 从人类行为到智能体行为**

在过去相当长的一段时间里，人类行为系统 \[623] 显著地推动了我们将计算机系统发展为自主范式的进程。行为机制在人类大脑中起着关键作用，用于驱动以目标为导向的行为。在智能人脑中 \[624]，有意识与无意识的思维信号被产生，并被转换为心理信号，最终引发一系列行动操作。这一过程可以被映射为一个多阶段的流程，涉及构建行动空间、设计学习机制以提升决策能力，以及整合外部状态（例如工具）。受到这些原理的启发，我们发现这些设计对构建 AI 智能体的原型至关重要。

目前已有许多框架在其设计中引入了行为学习，或将其作为输出目标。为明确行动系统的定义，我们对不同框架进行了区分，包括大型语言模型（LLM）、大型多模态模型（LMM）、机器人基础模型（RFM）以及大型行为模型（LAM），如表 8.1 所示。具体而言，LLM 是基于提供的提示生成语言输出，LMM 则是基于多模态输入生成多模态内容。现有的语言或数字类 AI 智能体框架，往往是基于这些基础模型（如 LLM 或 LMM）构建，并通过预定义的行动空间与学习策略来执行任务。

而 RFM（机器人基础模型）旨在基于真实环境（例如机器人视频）来优化机器人控制。这类模型通常预训练于网络规模的视频数据，并通过视频预测来模拟机器人的控制行为。尽管在构建物理 AI 智能体时已经涉及了一些行为设计，但 RFM 的核心依然是利用生成目标从大规模数据中学习知识。

此外，一些近期工作 \[622] 引入了“大型行为模型（LAM）”的概念，进一步强调生成行动策略、与真实环境交互，并增强自我学习范式的阶段。从这些定义中可以看出，无论底层使用的是哪种基础模型，行动系统的核心在于构建与环境的交互过程，并通过预定义的奖励函数从收集到的行为轨迹中进行学习。

具体而言，这些行为背后的机制与人类认知中的行动系统也存在高度相似性，为 AI 智能体框架中的行动系统设计提供了有价值的启发。例如：

* **在处理不同场景时，人类通常会预设一个行动空间，以执行行为轨迹来解决特定任务。**
  比如在玩《Minecraft》这类游戏时，我们会通过键盘或鼠标设置自己的操作行为，来模拟诸如建造房屋、开采黄金等行为。在此基础上，我们也需要在 AI 智能体框架中构建或创建一个可用于复杂任务的行动空间。

* **与机器相比，人类认知系统擅长通过现实世界的交互不断获取新知识，核心在于生成并优化行动序列。**
  因此，复刻这种学习能力对于 AI 智能体适应动态环境、构建新的技能库至关重要。

* **此外，随着人类文明的发展，学会使用外部工具被认为是人类智能进化的重大里程碑之一。**
  借助这些外部工具，人类在不同情境中的问题解决能力得到了极大的扩展——从石器时代到工业革命皆是如此。

因此，我们希望能够构建人类认知中的行为系统与 AI 智能体框架设计之间的映射关系，涵盖如下关键点：

1. 如何从特定场景扩展到通用领域，为 AI 智能体构建行动空间；
2. 如何在环境中建立行为学习机制；
3. 如何利用外部状态（如工具）拓展 AI 智能体的任务边界。

通过发展这样一项系统性的综述研究，我们希望为学术界和产业界提供更深入的见解，从而清晰理解“行为系统”在 AI 智能体框架中的重要性。



---

## 8.3 主动式智能体系统范式（Paradigms of Agentic Action System）

一般来说，AI 智能体框架中的行动系统主要由以下三个核心组成部分构成：

1. **行动空间 A（Action Space A）**：包含智能体在现实世界场景或下游任务中可以执行的各种动作类型，具体内容因不同的智能体设置而异，例如语言型智能体和具身型智能体；
2. **动态环境中的行动学习**：涉及状态 $S$、观测 $O$ 以及智能体的优化过程；
3. **工具空间 T（Tool Space T）**：包含智能体可调用的工具、接口或中间件，既可以是实体设备如机械臂，也可以是数字接口如 API。

总体而言，这些组件共同定义了 AI 智能体行动系统的范围与特性，影响其行为建模和执行能力。

为了在实际场景中充分挖掘可能的行动能力，我们需要形式化地表示行动空间，并同时考虑个体操作和潜在的分层推理过程。这意味着要从多个层次审视行动空间，从底层操作到高层策略运算，以协调复杂的工作流程。

因此，AI 智能体的决策过程可以形式化为一个轨迹 $\langle o_t, s_t, a_t \rangle$，其中动作 $a_t$ 从行动空间 A 中选出，并依据当前观测 $o_t$ 作用于当前状态 $s_t$，以转变为下一状态。在某些情况下，整合外部工具系统也是必要的。通过执行一系列 $\langle o_t, s_t, a_t \rangle$，智能体可以逐步实现最终目标。

---

### 8.3.1 行动空间范式（Action Space Paradigm）

行动空间 A 是构建 AI 智能体行动系统的基础，其构成方式决定了智能体在不同场景下解决复杂任务的能力。图 8.2 展示了一个基于行动空间的行动系统分类示意图。我们将现有工作中的行动空间总结为以下三类：

#### 1. **语言类（Language）**

语言型 AI 智能体通常通过语言驱动的动作在交互式语言环境中运行，如推理、编程、信息检索、执行 API 调用或与外部工具交互。我们将语言类行动空间分为三种类型：**纯文本、代码编程、通信交互**。

* **纯文本类**：如 ReAct，通过结合大语言模型的推理与行动能力解决问题；AutoGPT 将用户请求分解为子任务并通过搜索等工具处理；Reflexion 引入自我反思和记忆机制增强语言任务的行动执行；LLM+P 提升大语言模型的规划能力以辅助决策。但纯文本指令需先被解释再转化为可执行命令，易产生信息损失。

* **代码类**：直接以代码作为行动空间进行生成与验证，例如 MetaGPT 和 ChatDev 采用多智能体协作编程，SWE-Agent 关注软件工程流程，OpenDevin 建立自动软件开发平台，整合代码生成、命令交互、沙箱执行等环节。

* **多智能体通信类**：通过智能体间的对话决定下一步行动，如 Generative Agents 在虚拟城镇中模拟角色互动；MetaGPT、ChatDev 和 AutoGen 等均通过多智能体合作解决复杂任务。

语言型智能体在语言交互方面表现出色，但受限于行动空间，在现实复杂任务中的处理能力仍有挑战，因而需探索更复杂的行动空间结构。

---

#### 2. **数字类（Digital）**

为了扩展智能体的能力，部分研究发展了可在数字环境中运行的智能体，如网页代理、在线购物平台、游戏系统等：

* **MineDojo**：通过视频-语言预训练构建虚拟智能体，在 Minecraft 中完成多种任务；
* **Voyager**：训练成能在 Minecraft 中自主学习技能库的具身智能体；
* **JARVIS-1**：可处理多模态输入输出，制定复杂计划，执行具身控制；
* **SwarmBrain**：使用 LLM 在 StarCraft II 中进行战略性实时决策；
* **MM-ReAct、ViperGPT**：处理多模态任务，选择视觉专家；
* **Visual-ChatGPT、HuggingGPT**：集成多个视觉专家，由 LLM 控制任务执行；
* **WebGPT、WebAgent**：接入搜索引擎提高网页搜索与问答能力；
* **WebShop、Mind2Web**：用于模拟在线购物与复杂网页任务；
* **Mobile-Agent、AppAgent**：实现对移动设备功能与应用的控制；
* **UFO、OmniParser**：以 GUI 操作为行动空间，支持复杂电脑任务。

此外，LLM 与结构化数字环境的集成也被广泛研究：

* **Pangu** 连接 LLM 与大规模知识图谱；
* **BIRD、Spider 2.0** 使 LLM 能在企业级数据库中运行；
* **NL2SQL-BUGs** 改进自然语言到 SQL 转换中的语义错误识别；
* **UnifiedSKG、Middleware** 扩展 LLM 在数据库和知识图谱上的操作能力。

这些框架推动了 AI 智能体从“语言智能”向“数字智能”转型，扩展了网页浏览、GUI 交互、移动应用、具身系统等多模态行动能力。

---

#### 3. **物理类（Physical）**

构建能在物理世界中交互的 AI 智能体被视为最终目标，即模拟具有人类认知能力的程序。为此，智能体需能处理来自真实环境的连续信号，并生成反馈不断优化行为。

* **RT-family**：将网络视频知识注入到机器人学习中，预训练视觉-语言-行动模型；
* **GR-2**：基于大规模视频与语言数据进行预训练，并在机器人轨迹上微调；
* **π0**：支持单臂、双臂、移动操作器的机器人平台预训练；
* **SayCan**：将机器人语义与 LLM 连接，用 LLM 做高层决策；
* **VoxPoser**：使用 LLM 分解 3D 值图进行机器人操作；
* **EmbodiedGPT**：融合视觉与语言模型处理视频并进行决策性操作。

然而，与语言和计算机操作等离散动作不同，物理环境中需处理连续信号与动作，这对现有基础模型提出了巨大挑战。消除连续与离散信号之间的鸿沟仍是基础模型的一大难题。

---

### 总结

行动空间是构建高效 AI 智能体系统的关键组成部分。高效的行动空间能显著提升智能体在下游任务中的处理能力。行动空间可以从离散空间（如 Atari 游戏的技能库）扩展到连续空间（如机器人操作）。随着 AI 智能体日益具备自主性与多模态能力，设计更有效的行动空间将是推动通用 AI 系统实现现实交互的核心突破方向。

---

### **8.3.2 行动学习范式**

在人类认知系统中，**行动学习**（Action Learning）\[669] 代表了解决问题的过程，涉及采取行动和对反馈进行反思。同样地，AI 智能体的行动学习指的是其通过与真实环境的直接交互，不断迭代优化决策和行为的过程。通常，行动学习包含多个阶段的循环过程，包括构建行动空间、选择行动、以及基于与环境的交互（例如接收反馈或奖励并调整行动选择策略）来优化行动选择。通过反复部署这些策略，AI 智能体可以实时适应最新的信息或变化的环境条件，最终实现更强健、灵活和高效的问题解决能力。因此，构建一个有效的行动学习机制，对于智能体行动系统的优化至关重要。

在本节中，我们主要聚焦于三种代表性的学习范式：**上下文学习（In-context learning）**、**监督训练（Supervised training）** 和 **强化学习（Reinforcement learning）**，它们具体如下：

---

#### **一、上下文学习（In-context Learning）**

随着大语言模型（LLM）展现出涌现能力（emergent ability），上下文学习被认为是无需对模型进行修改即可充分利用其现有能力的最有效方法。通过设计良好的提示词（prompts）描述行动，AI 智能体可以理解特定行为，执行这些行为，并对与环境交互的结果进行反思，从而完成任务。

典型方法是使用提示技术指引 LLM 生成智能体行动，其中最具代表性的是 **Chain-of-Thought（CoT）** \[46] 提示，即通过“让我们一步一步思考”的方式，生成一系列中间推理步骤，系统性地探索可能的解决方案。**ReAct** \[70] 允许 LLM 在交互中生成推理轨迹和任务特定的动作，增强推理和决策能力。**LearnAct** \[652] 提出迭代学习策略，通过生成代码（如 Python）来扩展行动空间并修订行动。

另外，一些工作（如 Auto-CoT \[137]）探索了如何自动生成 CoT 以启用智能体的自主思考。为了处理更复杂的任务，**ToT** \[72] 将思考过程结构化为树形结构，引入树搜索；**GoT** \[75] 则使用图结构和图搜索机制。

针对机器人模型，**CoA** \[649] 设计了四种提示方式（如物体、抓取、空间、移动）以支持具备推理过程的机械臂操作。为了实现更复杂的任务流程，一些框架引入了任务分解阶段，如 **Least-to-Most** \[138] 将用户指令转换为多个子任务。**HuggingGPT** \[152] 是一个代表性的 AI 智能体框架，应用任务规划将用户需求转化为可执行步骤。**Plan-and-Solve** \[650] 直接利用 LLM 规划并基于计划回答问题，**Progprompt** \[93] 在机器人任务中也采用类似的任务分解思路。

此外，使用提示技术来构建 AI 智能体的行为特性也成为一大趋势，例如 **Generative Agents** \[50]、**MetaGPT** \[626]、**ChatDev** \[627]、**SWE-Agent** \[628]。还有如 **Reflexion** \[48] 与 **Self-refine** \[67] 等框架，会基于环境反馈，反复优化和改进结果。

上下文学习有助于避免参数微调，降低训练成本，能让智能体在多个领域高效执行任务。但如果要进一步增强智能体的行动学习能力，仍然面临诸多挑战。

---

#### **二、监督训练（Supervised Training）**

为进一步提升基础模型的行动学习能力，越来越多研究聚焦于包括**自监督预训练（PT）** 和 **监督微调（SFT）** 在内的训练方法。

在预训练范式中，最具代表性的是 **RT 系列** \[522, 643, 644]，即在大规模 Web 和机器人数据上预训练机器人 Transformer，从而获得强大的视觉-语言-动作模型。类似地，**GR-2** \[357] 在大规模 Web 视频上进行预训练以理解世界动态，并在机器人轨迹数据上进行微调；**LAM** \[622] 在用户与计算机交互的轨迹数据上进行训练。

但预训练通常计算资源消耗巨大，因此许多研究采用微调方式提升基础模型的动作能力。例如，**OpenVLA** \[670] 基于 Llama2 \[11] 构建，结合 DINOv2 \[671] 和 SigLIP \[672] 的视觉编码器，并在 Open X-Embodiment（OXE）\[673] 提供的真实机器人演示数据上微调，参数仅为 RT-2-X 的 1/7，却在多个任务中表现更优。

在 OpenVLA 的基础上，**CogACT** \[653] 引入扩散动作模块和自适应动作集成策略，在模拟环境 SIMPLER 中提升了 35%，在实际机器人任务中使用 Franka 机械臂提升了 55%。此外，**RT-H** \[654] 引入层次架构，先预测语言动作再生成底层动作；**π0** \[645] 从多种机器人平台收集数据，并在预训练视觉语言模型上进行微调；**UniAct** \[656] 学习通用动作，通过提取不同形态机器人之间的结构共性，实现跨领域、跨平台泛化。

综上，监督训练（包括预训练和微调）可以有效让基础模型适应现实世界的智能行为任务。同时，哪怕已经训练过模型，后续依旧可以结合上下文学习以进一步提升表现。

---

#### **三、强化学习（Reinforcement Learning, RL）**

在上下文学习和监督训练之外，智能体还需要通过与环境的持续交互，不断优化其行为策略，以经验、反馈或奖励为基础进行学习。由于这种过程的迭代性与顺序性，强化学习提供了系统化的方法论 \[675-678]。

经典算法包括 Deep Q-Network (DQN) \[679] 和 Proximal Policy Optimization (PPO) \[680]。在基础模型中最具代表性的 RL 应用是 **InstructGPT** \[43]，通过 RLHF（人类反馈强化学习）对大语言模型的输出进行对齐。但由于 RLHF 需要训练奖励模型，**DPO** \[111] 提出用对比学习直接优化偏好数据。

已有研究 \[89, 681] 展示了 RL 在生成长 CoT 推理流程中的潜力。虽然 RL 成功应用于文本生成任务 \[12, 682, 43, 683]，但在行为学习中的高效利用仍是挑战之一。当前的进展主要包括以下两方面：

#### 1. 结合 LLM 的世界知识进行环境模拟或生成想象轨迹：

* **RLFP** \[657] 使用策略模型、价值模型、奖励模型对智能体探索过程提供指导；
* **ELLM** \[658] 借助 LLM 的知识提升探索效率；
* **GenSim** \[659] 利用 LLM 的编程能力自动生成仿真环境与专家演示；
* **LEA** \[660] 利用 LLM 语言理解能力，将其适配为状态转换模型与奖励函数；
* **MLAQ** \[661] 通过 LLM 构建世界模型并使用 Q-learning 获取最优策略；
* **KALM** \[662] 微调 LLM 以双向转译文本目标与动作轨迹，通过离线 RL 学习获取“想象”中的知识。

此外，如 **Search-R1** \[685]、**R1-Searcher** \[686]、**RAGEN** \[687]、**OpenManus-RL** \[688] 正在探索如何使用 RL 对智能体模型在代理环境下的轨迹数据进行微调。

#### 2. 层次化强化学习（Hierarchical RL）：

* **When2Ask** \[663] 允许智能体向 LLM 请求高层次指导，LLM 提供计划选项，智能体基于此学习底层策略；
* **Eureka** \[664] 利用 LLM 生成具有人类水平的奖励函数，用于复杂任务如五指操控；
* **ArCHer** \[665] 使用 off-policy 算法学习高层次价值函数，从而间接指导低层策略；
* **LLaRP** \[666] 使 LLM 同时理解文本目标与视觉输入，并将输出转化为动作分布。

总体来看，强化学习能够将基础模型与环境交互结合，通过在线学习与世界模型整合，推动智能体行动系统迈向更高水平。

---

#### **总结**

总体而言，借助行动系统，AI 智能体在多个领域展现出了显著的决策能力。例如，行动学习让智能体自动理解图形界面（GUI）并执行各种操作，从而提升人类在计算机使用上的效率。在机器人任务中，行动系统帮助智能体实现了抓取物体、叠衣服、清洁桌面等复杂行为。

行业中也出现了以行动模型为基础的研究方向，如自动驾驶系统，通过结合大模型的人类理解能力，使其能感知真实世界并模拟人类驾驶员。

总之，行动学习赋予智能体与现实世界互动的能力，拓展了 AI 应用的边界，带来更多在真实场景中的落地机会。

---


### 8.3.3 基于工具的行动范式（Tool-Based Action Paradigm）

**工具学习是区分人类智慧与其他动物智慧的关键。** 自石器时代以来，人类通过使用工具提高了效率、生产力和创新能力。类似地，让AI智能体能够利用各种工具在数字和物理环境中运行，是实现类人智能的基础步骤。

**定义：** 在人工智能中，工具被定义为允许智能体与外部世界交互的接口、仪器或资源。例子包括网页搜索 \[632, 705, 97, 634]、数据库 \[706–709]、编程环境 \[710]、数据系统 \[711–713] 和天气预报系统 \[714]。通过将工具功能转换为自然语言或API格式，基础模型的任务解决范围得以扩展。

工具系统在AI中的演变大致可以分为以下阶段：最初，随着大语言模型的发展 \[2]，重点是将工具转化为可解释的形式（例如函数调用）。随后，多模态处理的进步将交互方式从对话转向图形用户界面（GUI）；最近，研究开始探索通过控制硬件（如机械臂、传感器）与物理世界交互的具身智能体。简而言之，基于工具的行为可以被视为用于辅助的外部行为形式。

---

### 工具分类

类似于行动空间，工具也可以按类型划分为多个类别。本部分我们主要总结了三大关键领域：**语言、数字和物理**，同时也探讨了工具学习在**科学发现等新兴领域**中的潜力：

#### • 语言类工具：

为了使AI能够使用外部工具，我们通常将工具表示为某种函数调用的形式，其中包括任务描述、工具参数及相应输出。这种表示方式让大语言模型（LLM）能够理解何时以及如何使用工具。

* **ToolFormer \[689]**：通过集成外部工具空间（如计算器、问答系统、搜索引擎、翻译、日历），扩展了语言模型的能力。
* **ToolLLM \[690]**：使用RapidAPI作为行动空间，并用深度优先搜索决策树算法来选择最合适的工具解决任务。
* **Gorilla \[691]**：在工具文档基础上微调LLM，用于生成API调用。
* **ToolkenGPT \[692]**：优化工具嵌入，使LLM可以从中检索工具。
* **GPT4tools \[693]、AnyTool \[694]**：通过构建自我指导数据集并在其上微调模型，以提升工具使用能力。

总体来说，语言类工具利用得益于LLMs的强大能力，其效果已被大量研究验证，从纯文本到函数调用再到代码编程都得到了广泛探索。

#### • 数字类工具：

随着LLMs在语言处理中的成功，研究者开始尝试将其任务范围扩展至数字领域（如多模态、网页搜索、GUI等）：

* **MM-ReAct \[497]、ViperGPT \[498]、Visual ChatGPT \[496]**：使用LLMs作为控制器，从而选择视觉专家来完成不同任务。
* **HuggingGPT \[152]、Chameleon \[153]**：LLMs先进行推理和规划，再决定使用哪些多模态工具。
* **WebGPT \[632]、WebAgent \[634]**：将搜索引擎与LLMs结合，增强解决复杂任务的能力。
* **Mobile-Agent \[635]、AppAgent \[636]**：分别引入GUI操作和App使用，扩展AI智能体在移动任务中的应用范围。

相比物理世界，数字环境中更容易采集和处理数据。因此，通过基础模型与数字环境的互动，我们可以开发出在计算机、手机等设备中的智能助手。

#### • 物理类工具：

* **RT-2 \[643]**：展示了利用视觉-语言工具进行语言引导的机器人操作。
* **TidyBot \[695]**：展示了LLMs如何适应清洁工具以满足个性化家务需求。
* **SayCan \[646]**：使用LLMs作为认知系统，结合机械臂与视觉感知完成任务。
* **SayPlan \[292]**：构建了3D场景图作为行动空间，设计了多种动作与工具用于3D模拟，然后使用LLMs进行任务规划。

在现实应用方面，如**外科机器人 \[715]** 提出了一种多模态LLM框架，用于机器人自主进行抽血等手术子任务。**自动驾驶系统 \[716, 717]** 也结合视觉语言模型与控制工具，实现可解释的导航。

相比其他任务，物理世界的应用面临最大的挑战，但也具有最高的产业价值，因此需要持续探索更先进的行动学习与工具集成。

#### • 科学类工具：

科学工具在促进AI跨学科发展中发挥了变革性作用，使AI智能体能够学习、适应并执行任务。

* **HoneyComb \[696]**：在材料科学中展示了工具驱动的进展，其ToolHub提供对实时信息和最新文献的动态访问。
* **Material Science Tools**：在Python REPL环境中动态生成和执行代码，用于精确数值分析。
* **ChemCrow \[697]**：集成GPT-4和18个专家设计的工具，自动化复杂任务，如有机合成、药物发现、材料设计。使用工具包括OPSIN（结构转换）、计算器等。
* **SciToolAgent \[698]**：整合500多个工具（如Web API、ML模型、函数调用、数据库等），实现科学研究的范式变革。
* **SciAgent \[699]**：多智能体框架，结合本体知识图与专用代理，用于假设生成和关键分析，加速材料科学等领域的发现。

这些示例强调了将专业工具集成到AI框架中的变革潜力，有效应对特定领域的挑战。

---

### 工具学习的三大关键阶段（受人类进化启发 \[718]）：

1. **工具发现（Tool Discovery）**
   在现实环境中，存在大量从数字到物理的工具，如何找到最合适的工具来满足用户指令是关键挑战。该阶段要求AI智能体具备对用户指令的深刻理解以及对工具世界知识的认知。
   常见方法分为两类：

   * **检索式方法**：如HuggingGPT \[152]中，LLMs作为控制器进行任务规划，然后调用合适模型。
   * **生成式方法**：如ToolFormer \[689] 收集带有API调用的大量语料进行训练，ToolLLM \[690] 基于解决路径构建工具指令并微调模型。

2. **工具创造（Tool Creation）**
   除了使用现有工具，创造新工具的能力也是关键能力。对语言智能体而言，常见方式是利用LLMs生成可执行程序（包含代码与文档）。

   * **PAL \[701]**：将程序作为中间推理步骤生成。
   * **LATM \[702]、Creator \[703]**：生成代码并设计验证器验证生成工具。
   * **SciAgent \[699]**：不仅整合了多种科学工具，还构造了新工具用于科学发现。

3. **工具使用（Tool Usage）**
   有效使用工具是AI智能体能力的基石，使其能够在虚拟与物理世界之间架起桥梁。其扩展包括：

   * **纵向专业化**：在机器人、科学、医疗等复杂领域利用专业工具。
   * **横向集成**：结合多模态工具包（视觉、语言、控制）解决问题。
   * **具身化（Embodiment）**：通过机器人工具和传感器与环境进行物理互动。

---

### 总结：

工具学习与行动学习共同构成了AI智能体中行动系统的两个核心部分。工具学习可被视为一种使用外部状态进行问题求解的行为方式。通过API或函数调用，语言模型能够直接复用现有模型的能力（如检索、编程、搜索），而非仅依赖下一个词的预测 \[719]。

工具学习涵盖多个挑战性阶段，包括如何确定工具空间、发现与选择工具、创造与使用工具。整体来看，**工具学习在构建能解决多领域复杂任务的通用AI智能体框架中扮演着核心角色。**

---

## **8.4 行动与感知：“自外而内”还是“自内而外”**

[插图]
在认知科学和神经科学领域，有一个核心争论：在智能系统中，是行动还是感知处于因果流的根源？图 8.5 展示了不同的观点。

传统的“自外而内”（outside-in）视角认为，因果影响始于外部刺激。环境激活外围感受器，这些信号向内传播，最终产生行为。这一视角将有机体——或称智能体——描绘为本质上是**被动反应**的：外部世界引起感官变化，智能体的行为只是这些变化的下游效应。

与此相对，Buzsáki 提出的“自内而外”（inside-out）框架 \[18] 认为，是智能体自身的行动塑造了输入信号的意义和后果。这种观点意味着智能体是**主动的**：它持续生成预测和运动指令，同时向感官区域发送“伴随放电”（corollary discharge）或“动作副本”。这些内部生成的信号作为参考，让智能体能够判断哪些感官变化是由自身引起的，哪些是由外界施加的。这样一来，因果性就从外部事件转移到了内部发起的行为上，而外部刺激则只起到确认或校正的作用。

这种因果的反转对于我们如何理解感知的目的与功能有重大意义：感知不再是一个终点，而是更新与修正智能体自身关于环境的**行动驱动假设**的手段。

从进化角度看，具备在无需复杂感知分析的情况下运动的能力，可以带来即时的生存优势。即便是简单的生物，只要能定期运动搅动富营养水体，就可能获益良多，而这早在人类复杂的感知系统进化出来之前就已存在。换句话说，在进化时间线上，**运动先于高级感知**，这表明行动能力并不只是对外部刺激的被动回应，它本身也可能是推动后续感知系统发展的主因。正是当行动机制发展到一定程度时，智能体才真正从额外的感知器官中受益，这些感知器官可以更具策略性地引导运动。

这种发展顺序将感知的演化基础扎根于其实用性，将感知辨别与运动所带来的实际结果联系起来。

**行动与感知关系紊乱时，能揭示它们之间复杂的因果环路。**在“睡眠瘫痪”中，大脑发出的运动指令暂时无法传递到肌肉上；外部刺激依旧轰炸感官，但正常的“行动-感知”校准却丢失了。因此，个体会产生强烈的不真实感，因为大脑缺乏内部参考信号去解释感官输入。同样地，如果人为外力推动眼球，而非大脑发出运动指令，视觉场景也会出现运动错觉，凸显出感知在缺乏**自我发起的先导动作**时容易产生混淆。

神经生理学数据也进一步支持“自内而外”模型：许多传统认为是“纯感官区”的神经元，不仅响应外部刺激变化，也会跟踪自我发起的动作——有时甚至反应更强。这表明，大脑中的“因”往往起源于内部，指导外部信号的强度与意义。没有这些内部对应，原始感官数据对于系统而言可能变得模糊甚至无用。

---

### **对智能体的启示**

“自内而外”的观点为现代智能体研究提供了有力洞见。当今大多数 AI 系统——包括很多大语言模型（LLM）代理——仍主要以**反应式**模式运作：它们等待用户输入，并基于从海量数据中学习的统计关联生成回应。这种被动行为正如“自外而内”框架，智能体的角色局限于回应，而非主动发起。

但如果智能体可以主动行动，持续通过\*\*自发行为（无论是物理上的还是表征层面的）\*\*形成并验证假设，那么它就能赋予自己的“感知输入”以意义——无论是来自感官流还是语言提示，从而减少歧义。例如，一个基于 LLM 的代理如果能主动提问或验证自己的陈述是否与知识库一致，它就能更好地区分哪些推理是自身引起的，哪些是由外部数据强加的。通过追踪这些自发贡献（类似于伴随放电），模型可以提升连贯性、减少“幻觉”错误，并通过迭代的因果循环改善其内部状态。

**采取主动立场**还可以实现更高效、情境感知更强的学习。不再被动等待标注数据，智能体可以探索环境、引发反馈，并将自我生成的经验纳入训练之中。随着时间推移，这种紧密耦合的“行动—感知”回路将增强智能体应对复杂任务、适应突发挑战并实现更强泛化能力的能力。

从“自外而内”到“自内而外”的转变，重塑了感知在因果链条中的地位——它是行动的结果，而非起点。无论是生物体还是人工智能系统，都可能从中受益：通过承认“行动”的因果力量，并致力于构建**主动的**而非仅仅是被动的智能体，我们可能更接近于理解自然认知机制，也更接近于打造新一代智能系统。

---

以下是表格 Table 8.2 的中文翻译和还原：

| 维度          | 人类大脑 / 认知                                                                  | 大型语言模型（LLM）代理                                                         | 备注                                                         |
| ----------- | -------------------------------------------------------------------------- | --------------------------------------------------------------------- | ---------------------------------------------------------- |
| **感知**      | - 整合多种感官通道（视觉、听觉、嗅觉、触觉、味觉）。<br>- 感知与情绪、内分泌系统和身体状态密切相关。<br>- 高度敏感，能感知细微差异。  | - 主要基于语言，具备一定的多模态能力。<br>- 感知依赖于外部传感器和模型，整合能力有限。<br>- 缺乏与身体状态的实时耦合。    | 感知差异导致对现实理解方式的不同。具身智能（Embodied AI）试图弥合这一差距，但在硬件和软件层面仍面临挑战。 |
| **统一表示**    | - 同时处理多模态输入：视觉、听觉、语言、动作和情绪。<br>- 不同脑区协同作用，形成统一的时空和语义理解。                    | - 主要基于文本。一些多模态模型可处理图像或音频，但整合度较低。<br>- 缺乏像人脑一样的完整时空建模能力。               | 即便是先进的多模态模型，也不具备人脑的整体统一表示能力。当前仍存在硬件与算法挑战。                  |
| **任务切换的粒度** | - 能灵活在宏观与微观认知任务间切换。<br>- 能进行高层次规划，并在需要时关注细节。<br>- 根据上下文和工作记忆动态调整任务优先级和关注点。 | - 严重依赖提示工程来控制任务粒度。<br>- 无法自主在不同任务层级间重新分配注意力。<br>- 缺乏提示指导时，可能卡在某一抽象层级。 | 人类可以根据情境动态调整认知粒度，而LLM需要明确指令才能有效切换任务焦点。                     |
| **行动**      | - 以目标为导向，整合多感官信息做出决策。<br>- 能通过与环境交互实现实时学习。<br>- 包括身体活动和心理过程。               | - 行动空间需提前定义。<br>- 无法支持连续空间中的动作。<br>- 依赖在线训练来优化决策过程。                   | 人类能够主动学习新动作并执行连续性动作，而当前的LLM代理尚不具备这种能力。                     |


---

### 8.5 总结与讨论

传统上，行动代表人类认知系统在与环境交互反馈基础上的行为表现。它赋予人类思考、推理、说话、奔跑以及执行各种复杂操作的能力。基于这一行动系统，人类可以通过从世界中不断增强感知和行动能力，从而迭代进化大脑智能，形成一个闭环，并进一步在世界中创造出新的文明与创新。类似于人类认知系统，行动系统与工具系统在 AI 智能体中也扮演着重要角色。集成行动系统能够帮助 AI 智能体系统性地进行规划、执行和调整行为，从而在动态环境中展现出更强的适应性和鲁棒性。

本节中，我们系统性地审视和总结了行动模块对 AI 智能体的影响，主要围绕**行动系统**和**工具系统**两个方面展开。

#### 行动系统

我们从三个角度简要描述了行动系统：**行动空间**、**行动学习**和**工具学习**。

在行动系统中，**行动空间**通常是最关键的组成部分，决定了 AI 智能体在解决下游任务时的上限。它定义了在与现实环境交互过程中，AI 智能体可以选择并执行哪些动作。行动空间的构建也存在诸多困难，具体取决于数据类型，从离散到连续数据不等。随着对 AI 智能体需求的不断增长，智能体也被期望能处理更复杂的任务，尤其是涉及现实应用的任务。因此，如何构建一个鲁棒且通用的行动空间仍是一项持续的挑战。

在此基础上，**行动学习**是使智能体能够有效与外部世界和人类互动的另一关键组成部分。它代表了智能体在与现实环境交互过程中学习和优化其策略的过程。基于不同的基础模型，也衍生出了不同的行动学习范式，如零样本学习（例如提示工程）、监督学习和强化学习。在行动学习中，关键在于深入理解任务，包括如何设计系统提示词、如何确定预训练或微调数据集，以及在训练过程中使用何种奖励信号或优化策略。

尽管行动学习在推进 AI 智能体框架方面取得了显著进展，仍有许多问题有待解决。例如，ICL（in-context learning）范式需要特定的先验知识来设计合适的提示词；此外，结合预训练和微调的监督学习需要高质量、多样化的数据，这通常依赖于精细的数据处理和大量人工投入；强化学习的不稳定性也使其在大规模训练中的应用存在挑战。

此外，行动系统的设计对工具系统的整合效果具有决定性影响。一个有效的行动系统可以帮助智能体无缝对接各种工具，执行复杂的用户意图，并将外部数据转化为有意义的输出。这种“行动系统+工具”的协同不仅减轻了记忆的限制，降低了幻觉风险，还增强了系统的专业能力和鲁棒性。

例如，一个配备了强大行动系统的 AI 智能体可以动态选择并调用最合适的工具来完成某个任务，确保响应的准确性和效率。同时，行动系统还能促进分层推理流程，使智能体能够组织复杂的工作流，与用户目标高度对齐。这对于需要精确执行和实时决策的任务至关重要，从而在基础模型能力与实际应用需求之间搭建起桥梁。

此外，工具执行过程的透明性和可解释性也提升了用户信任，促进了高效的人机协作。因此，专业工具与强大行动系统的结合，在多样且动态的环境中极大地提升了 AI 智能体的性能、可靠性和适用性。

**总之**，行动系统能够显著奠定 AI 智能体框架的问题解决能力基础，使其可以处理超出基础模型范围的复杂任务。

---

### 未来方向

尽管如此，构建一个有效的智能体行动系统仍需解决诸多挑战，我们总结如下：

1. **效率** 是一个重大障碍，尤其在实时应用中，快速而准确的响应至关重要。行动系统的复杂性可能导致无法接受的延迟，从而阻碍在欺诈检测或实时决策等场景中的部署。为减缓这些效率问题，需要采取一些策略，如过滤无关或冗余信息、利用零样本提示来简化推理过程、使用高速缓存存储解决方案来缓存相关知识等，以确保性能的同时减少响应时间。

2. **评估** 也是行动系统中的关键因素，包括行动学习和工具学习两个方面。在现实环境中，存在大量来自不同来源的动作。因此，如何从不同来源中选择正确的动作或工具、避免信息冲突，是当前 AI 智能体面临的重大挑战。为了缓解这些问题，构建有效且鲁棒的评估体系，以评估行动系统的准确性和可靠性至关重要。发展评估机制、验证协议、以及透明的评估方法将有助于降低动作预测的不准确性。此外，揭示基础模型的决策过程也有助于理解哪种动作更优，以及如何协调各种动作或工具以输出可信结果。

3. **多模态行动学习** 在基于 LLM 的自主智能体中已取得显著进展，得益于大语言模型的发展。但如何理解并调用超出语言指令的行动（如 GUI 操作或具身工具）仍然是挑战。在现实场景中，人类可以通过语言、图像、视频或人工引导等多种方式学习并发展新技能。因此，让 AI 智能体能通过多种模态来发展或学习行动，是提升其现实任务处理能力的关键。换句话说，我们亟需探索如何缩小人类与 AI 智能体在工具使用能力上的差距，进而推动更先进的智能体框架设计。

4. **隐私问题** 是生成式 AI 尤其是 LLM 中的重要关注点。因此，保护用户敏感数据的隐私、防止用户行为泄露，在工具调用过程中至关重要。为解决这些问题，可以使用如联邦学习等安全技术，使模型能在分布式数据源上进行训练，而不直接暴露敏感信息。此外，还常常需要使用模型蒸馏技术，以在确保性能的同时保障数据完整性。这些方法使得在保护用户数据隐私的同时，仍能高效训练模型。

5. **安全性** 也同样重要。人类与模型协作中的伦理问题，以及模型与物理环境交互时的安全问题，都需慎重考虑。在将人类劳动与 AI 系统融合时，必须保障人的尊严与自主性。建立伦理规范、保障公平的工作条件、推动跨学科合作，都是必要步骤。此外，开发强大的安全机制，以防止 AI 系统在物理工具或行为交互中的错误或恶意行为，也至关重要，以保障潜在风险不被放大。

除了上述挑战，行动系统仍存在一些尚未解决的开放问题。例如，如何在基础模型和外部工具之间实现最佳平衡，决定在何时使用前者、何时依赖后者，仍没有明确答案。尽管工具系统为基础模型提供了灵活性和扩展性，但近年来越来越多的趋势是增强基础模型本身的能力。因此，在设计通用、高效的 AI 智能体时，如何平衡基础模型与工具系统之间的使用，是一项核心课题。




