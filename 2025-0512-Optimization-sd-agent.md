# Optimization self-evaluation



自主智能体的优化是一个涉及多层抽象的复杂挑战。本节首先将提示词优化（Prompt Optimization）确立为基础层，进而衍生出三条不同的优化分支：智能体工作流优化、工具优化以及全面自主智能体优化。

### 9.1 智能体优化概述

现有基于大语言模型（LLM）的智能体优化可以被构想为一个两层架构。底层是提示词优化，专注于提升语言模型节点的基本交互模式。在此基础上，出现三条并行分支：

1. **工作流级优化**：聚焦于多个语言模型节点之间的协调与交互模式；
2. **工具优化**：智能体通过开发和改进工具来适应新任务并利用历史数据实现进化；
3. **全面自主智能体优化**：通过多维度考量实现智能体能力的整体增强。

类似于 AutoML 中的优化范式，智能体优化可以分为单目标优化和多目标优化。目前的优化主要集中在三个经典指标上：性能、推理成本与延迟。性能衡量智能体完成任务的效果，推理成本量化了智能体运行所需的计算资源，而延迟表示智能体响应和完成任务所需的时间。

在不同优化模式中，这些目标可能有所不同。例如，在提示词层级的优化中，提示词长度可能会成为额外的优化目标。这种多维度的优化目标反映出智能体系统的复杂性及其对多重竞争性需求的权衡。

---

### 9.2 提示词优化

在基于大语言模型的智能体优化中，提示词优化起着至关重要的作用。除了模型层级的优化之外，针对特定任务或模型的提示词优化会直接影响智能体的性能、延迟和成本。

#### 9.2.1 评估函数

评估函数 \$\phi\_{\text{eval}}\$ 是提示词优化的核心，负责生成优化信号并引导提示词的进化。它在评估来源、评估方法与信号生成之间建立反馈回路，实现持续优化。以下三类维度定义了其结构：

**评估来源**

* 主要包括 LLM 生成的输出 \$G\_{\text{llm}}\$ 和任务真实结果 \$G\_t\$。
* 大多数研究采用两者的比较进行评估；部分方法仅使用 \$G\_{\text{llm}}\$。
* 示例包括：PROMST 利用人类规则对 \$G\_{\text{llm}}\$ 评估；SPO 使用不同提示词输出的对比来判断有效性。

**评估方法**

1. **基准评估**：使用预定义指标或规则自动评分，是最常见方法；
2. **LLM 作为评估者（LLM-as-a-Judge）**：利用 LLM 的偏好对齐能力和评估标准自动评估，例如 ProteGi、TextGrad、Revolve；
3. **人工反馈**：直接由人类评估，虽然成本高，但适用于开放任务，能快速提升提示词质量，如 APOHF 方法。

**信号类型**

* **数值型反馈**：以标量形式呈现性能，如准确率等，适用于规则、人评与 LLM 评分；
* **文本型反馈**：提供具体、细致的分析和建议，适合需要实例级优化的情况；
* **排序型反馈**：通过全面或成对排序判断相对优劣，适用于难以定义绝对指标的任务。

---

#### 9.2.2 优化函数

优化函数的设计决定了每一轮提示词生成的质量。通过高效的信号引导，可加快收敛速度。目前主要有两类优化策略：

**基于评估信号的优化**

* 首先选出当前最优提示词，再用启发式方式进行优化。
* 如 SPO 使用当前最优提示词的输出进行微调；
* EvoPrompt 用进化算法和语言模型作为变异操作；
* PromptBreeder 比较变异提示词的得分变化，优化元提示词与提示词本身。

**基于优化信号的优化**

* 明确提取高性能提示词的共性以指导后续优化；
* OPRO 从高分提示中提取模式；
* ProTegi 分析失败案例并预测错误原因；
* TextGrad 将反思转化为“文本梯度”，用于多提示词优化；
* Revolve 模拟二阶优化，通过比较前后梯度变化，避免陷入局部最优。

---

#### 9.2.3 评估指标

提示词优化方法的评估可从以下三方面进行：

1. **性能指标**：如 pass\@1、准确率、F1 分数、ROUGE-L，用于衡量任务完成效果及优化稳定性；
2. **效率指标**：评估所需资源，反映性能与计算成本的权衡；
3. **行为指标**：

   * 一致性：多次运行结果的稳定性；
   * 公平性：缓解模型偏见的能力；
   * 置信度：智能体对其预测的信心。

这些行为维度如果作为独立目标，则提示词优化框架也需提供相应的评估方法。

---

### 9.3 工作流优化

尽管在提示层级的优化已经在提升单个大语言模型（LLM）能力方面展现出良好效果，但现代 AI 系统往往需要多个 LLM 组件的协同，以解决复杂任务。这就需要一个更全面的优化领域——代理式工作流空间（agentic workflow space）。

在本质上，代理式工作流由调用 LLM 的节点构成，每个节点代表一个为特定子任务设计的专用 LLM 组件。

虽然这种架构与多智能体系统类似，但需要将代理式工作流与完全自主的多智能体场景区分开来。在代理式工作流中，各节点依照预设的协议和优化目标运行，而不是具备自主决策能力。

许多著名系统，如 MetaGPT \[626] 和 AlphaCodium \[786]，都可以归类于这种架构。此外，代理式工作流还可以作为更大型自治代理系统中的可执行组件，其优化对于推动专用任务执行和通用智能体能力的提升都至关重要。

根据 GPTSwarm \[651] 和 AFLOW \[773] 提出的形式化方案，本节首先建立了代理式工作流及其优化目标的形式定义。随后，我们将分析代理式工作流的核心组件——节点与边——探讨它们各自的搜索空间，并讨论已有文献中的表示方法。

---

### 9.3.1 工作流的形式化

𝒦 是工作流的搜索空间，L(K, T) 通常衡量多个方面，包括任务完成质量、计算效率和执行延迟。该优化目标反映了部署代理式工作流时所面临的实际挑战：需在效果和资源之间权衡。

---

### 9.3.2 工作流边的优化

边空间 E 决定了代理式工作流的表示形式。目前的研究主要采用三种不同的表示范式：基于图、基于神经网络和基于代码的结构。每种范式都具备独特优势，并对优化过程带来不同的约束。

**图结构表示** 能够表达节点之间的层级、顺序和并行关系。这种方法天然适用于复杂的分支结构，方便工作流拓扑结构的可视化，尤其适合需要显式结构操控的场景。例如，GPTSwarm \[651] 通过拓扑感知优化，展示了图结构在协调多个 LLM 组件中的有效性。

**神经网络结构** 是另一种强大的表示方式，擅长捕捉节点之间的非线性关系。Dylan \[725] 显示，基于神经网络的工作流可通过可学习参数实现自适应行为，尤其适用于需要根据输入和反馈动态调整的场景。

**代码结构表示** 是当前表达能力最强的方法。AFLOW \[773] 和 ADAS \[741] 证明，将工作流表示为可执行代码支持线性流程、条件逻辑、循环结构，以及图与神经网络结构的集成。这种方式可实现对执行流程的精确控制，并利用 LLM 的代码生成能力。

边空间表示的选择会显著影响搜索空间的维度和可用的优化算法。例如，\[728] 在保持固定工作流拓扑的前提下，仅优化提示词，从而可使用基于文本反馈的优化方法。而 \[651] 则开发了可同时优化节点提示词和整体拓扑的强化学习算法。\[773] 利用基于代码的表示，使得语言模型可直接进行工作流优化；而 \[787] 和 \[788] 的最新研究则引入了问题特定的拓扑优化方法。

---

### 9.3.3 工作流节点的优化

节点空间 N 包含影响节点行为和性能的四个关键维度：

* **输出格式空间 F**：通过结构化 LLM 输出（如 XML 和 JSON 格式），可精确控制响应结构，对性能影响显著。
* **温度参数 τ**：控制输出的随机性，影响节点响应的稳定性与创造性之间的平衡。
* **提示词空间 P**：继承了提示级优化领域，决定了与 LLM 的核心交互模式。
* **模型空间 M**：表示可用的 LLM，每种模型具备不同能力和计算成本。

对于单个节点的优化，现有研究主要集中在节点空间中的某些维度上。\[773] 专注于提示词的优化，\[741] 将搜索空间扩展至提示词与温度参数的联合优化。\[789] 则在固定提示词的前提下，探索不同节点的模型选择。尽管输出格式优化非常关键，但目前研究较少涉及 \[790]。

与边空间优化相比，节点空间优化在可扩展性上面临独特挑战，因为代理式工作流通常包含大量节点。搜索空间的维度会随着每增加一个节点呈乘法增长，因此需要高效的优化策略，以在控制计算成本的同时，有效应对复杂性。

---

### 9.4 工具优化

与通常采用单轮交互方式的大模型不同，智能体具备更高级的多轮规划能力，并能够通过多种工具与外部世界进行交互。这些独特的特性使得工具使用的优化成为提升智能体整体性能与适应性的关键组成部分。工具优化指的是系统性地评估和改进智能体在选择、调用和整合可用工具以更高效率、更低延迟解决问题的能力。此类优化中的关键性能指标包括：决策准确性、检索效率、选择精度、任务规划能力与风险管理水平。

这一优化过程主要围绕两种互补策略展开：工具学习与工具创建。

---

### 9.4.1 学习使用工具

与依赖冻结大模型的提示式方法不同，基于训练的方法通过监督优化智能体背后的模型。受发展心理学启发，工具学习可分为两类：**演示学习**与**反馈学习**。

* **演示学习**是通过模仿专家行为进行的示范学习，例如行为克隆（Behavior Cloning），即通过监督方式让模型学习人类注释的工具使用操作。形式上，给定数据集 D = {(qi, a*\_i)}，其中 qi 是用户查询，a*\_i 是人类演示操作，控制器的参数 θC 通过以下目标进行优化：

  ```
  θ*C = arg max_θC E(qi, a*_i)∈D ∏_t=0^Ti p_θC(a*_i,t | xi,t, Hi,t, qi)
  ```

  其中，a\*\_i,t 表示时刻 t 对查询 qi 的人类注释，Ti 为总时间步。

* **反馈学习**则通过强化学习（Reinforcement Learning）使用环境或人类反馈中的奖励来引导模型优化策略。其目标函数为：

  ```
  θ*C = arg max_θC E_qi∈Q E_{ai,t}_{t=0}^{Ti} [R({ai,t})]
  ```

  其中 R 表示依据动作序列的奖励函数。

通过整合示范学习与反馈学习，智能体可逐步优化工具调用策略、选择策略与执行精度，提升在多样任务与环境中的泛化能力。

**优化工具使用的推理策略**包括：连锁思维（Chain-of-Thought, CoT）、思维树（Tree-of-Thought）与深度优先搜索决策树（DFS-DT）等，这些策略能够引导更复杂的决策过程。通过微调模型对工具的理解（如参数解释与动作执行），进一步提升交互精度。此外，对模型输出进行学习可改进后处理与分析，进一步提高工具使用效率。

---

### 9.4.2 创建新工具

除了优化已有工具外，基于任务理解和当前工具使用情况动态创建新工具的能力，也能显著增强 LLM Agent 框架的适应性与效率。

近期提出了多种互补方法：

* **ToolMakers** 建立了一个闭环框架，包括：

  1. 通过示例编程提出 Python 函数（基于 3 个示例）；
  2. 使用自动单元测试验证功能（3 个验证样本）并自我调试；
  3. 将验证通过的工具包装为可供下游任务使用的演示。
     此流程在保持全自动化的同时确保了可靠性。

* **CREATOR** 采用四阶段生命周期：

  * 抽象推理创建任务工具、
  * 工具调用的决策规划、
  * 程序执行、
  * 工具迭代纠错。
    强调工具多样性、抽象与具体推理的分离，以及容错机制。

* **CRAFT** 使用离线范式，通过 GPT-4 提示、验证与去重流程将领域特定数据提炼为可重用的原子工具（如对象颜色检测），无需训练，结合人类可审查代码与组合式问题求解，实现可解释的工具链。

这些方法的融合提供了丰富的研究机会。例如，未来的混合系统可将 CRAFT 的预制工具与 ToolMakers 的按需生成结合，通过函数缓存平衡效率与适应性。工具层级结构可能包括：CRAFT 提供基础操作，ToolMakers 构建组合工具，CREATOR 风格的纠错机制用于边缘案例。自监督工具评估指标与跨领域泛化的进步也将推动工具生命周期进一步自动化。

尤其值得研究的是工具的粒度（原子 vs 复合）与可复用性模式之间的关系：粒度越小，组合越灵活，但也增加了编排复杂性。随着智能体进化，未来可能出现任务与工具的双向共适机制：工具重塑任务表示，任务推动工具创新，从而实现自我改进的 AI 系统。

---

### 9.4.3 工具有效性评估

以下指标与评测基准为衡量智能体工具使用能力提供了全面依据。这些指标不仅衡量当前性能，还为工具使用优化提供明确目标，指导系统的即时增强与长期改进。

#### 工具使用评测基准

近年来，LLM 作为 Agent 的研究产生了多个评估工具使用能力的基准体系：

* **Gorilla**、**API-Bank**：创建了大规模数据集用于测试与外部 API 的交互。
* **T-Bench**、**ToolBench**：扩展了任务集，强调系统化数据生成。
* **StableToolBench**：关注真实 API 的不稳定性，引入虚拟 API 服务。
* **ToolAlpaca**：探索小模型在最小训练下的工具泛化能力。
* **ToolEmu**：通过沙箱环境评估工具增强系统的安全与风险。
* **MetaTool**：关注模型是否知道何时使用工具、选择哪个工具，推出 ToolE 数据集（包含单工具与多工具场景）。
* **ToolEyes**：考察多步推理与大规模工具库。
* **τ-bench**：引入用户在环（human-in-the-loop），关注动态交互与合规性。

这些基准标志着研究从孤立推理任务向真实世界智能体评估的转变。

---

#### 工具调用的评估指标

判断是否需要调用外部工具是影响系统效率与效果的关键步骤。给定标注数据集：

```
D_inv = {(qi, yi)}, 其中 yi ∈ {0,1} 表示是否需要调用工具
```

模型学习一个判决函数 d(qi)，定义为：

```
d(qi) = 1, 若 Pθ(y=1 | qi) ≥ τ  
        0, 否则
```

其中 Pθ 为模型预测调用必要性的概率，τ 是阈值。

常用评估指标包括：调用准确率 A\_inv、精确率、召回率、F1 值。若 C\_inv 表示工具调用成本，R(qi) 表示正确调用获得的收益，可定义净收益指标：

```
B_inv = ∑(1{d(qi)=1}·R(qi) − C_inv)
```

---

#### 工具选择的评估指标

调用工具后需从候选集中选择最合适的工具。设工具集为：

```
T = {t1, t2, ..., tM}
```

对于查询 qi，理想工具为 t\*\_i，模型选择为 t̂\_i，选择准确率为：

```
A_S = (1/|Q|) ∑ 1{t̂_i = t*_i}
```

也可使用排名指标如 MRR、nDCG 来衡量检索系统性能。

---

#### 工具检索效率与分层准确率

工具检索涉及识别工具的速度与准确性：

* 检索准确率（A\_R）使用 EM、F1 评估；
* 选择准确率（A\_S）评估最终选择的正确性；
* 工具使用意识综合评估精度、召回率、F1 值。

综合检索效率定义为：

```
E_Ret = (A_R × A_S × A_P × A_U) / C_R  
```

其中 C\_R 为检索成本，A\_P 表示规划准确性，A\_U 为工具调用成功率。

**MetaTool** 引入 Correct Selection Rate (CSR)，评估模型在四类场景下是否选对工具：

1. 相似工具中正确选择；
2. 上下文相关场景中选择恰当工具；
3. 避免选择错误或不存在的工具；
4. 多工具协同使用任务中的正确选择。

---

**9.5 面向自主智能体系统的优化**

除了在智能体演化过程中优化提示词、工具和工作流等单独模块（这些模块容易陷入局部最优，从而影响整个智能体系统的整体性能）之外，大量研究开始聚焦于**对整个智能体系统多个组件的整体优化**。这种**全局化的方法**使得大语言模型（LLM）驱动的智能体能够更全面地进化。然而，优化整个系统对算法提出了更高的要求——不仅要考虑单个组件对系统性能的影响，还必须考虑**不同组件之间复杂的相互作用**。

**ADAS \[741]** 是最具代表性的工作之一，首次正式定义了“智能体系统自动化设计”的研究问题。它将多个智能体系统组件整合进一个演化优化流程中。具体而言，ADAS 引入了一个**元智能体（meta-agent）**，可以在整体优化过程中迭代地设计智能体系统的工作流、提示词以及潜在工具。实验表明，ADAS 自动设计的智能体系统优于最先进的人工设计方案。

此外，\[726] 提出了一个**基于符号学习的智能体训练框架**，灵感来自神经网络中连接主义的学习原理。该框架将智能体管道类比为计算图，引入了一种**基于语言的反向传播和权重更新方法**。它定义了一个基于提示词的损失函数，通过智能体的执行轨迹传播语言损失，并相应地更新符号组件。这种方法支持结构化地优化智能体工作流，并可自然扩展到多智能体系统——可以将节点视为独立智能体，或允许多个智能体在单个节点中协同工作。

\[799] 提出了一个优化**提示词和智能体自身代码**的方法，使得智能体具备自我改进能力。这与“自指”理念相契合，即系统能够分析并修改自己的结构以提升性能。类似地，\[773]、\[787]、\[800] 和 \[788] 也关注对智能体系统中**工作流和提示词的联合优化**。其中，\[285] 提出一种方法，训练额外的大语言模型（LLM）来生成提示词和工作流，从而实现**自动化的智能体系统架构设计**。

总之，优化整个智能体系统的工作流并不是对各个组件优化的简单叠加。它需要精心设计的算法，以处理组件间复杂的依赖关系。因此，系统级优化是一个更加艰巨的任务，需要**更先进的技术手段**才能实现高效、全面的性能提升。

---

**将大语言模型视为优化器**

在本章中，我们介绍并探讨了将大语言模型（LLMs）视为**优化器**的一系列现有研究。首先需要指出的是，目前大多数研究主要集中于等式（9.1）中定义的**提示词优化问题**，因为对智能体工作流中其他组件的优化仍属于新兴研究领域。为了更好地理解这一方向，我们将其与经典的迭代算法进行类比，并考察它们在现代优化流程中的融合方式。

---

### **10.1 优化范式**

传统优化方法在**目标函数可访问性**方面存在不同的假设。我们可以将这些方法大致分为三类，每类所处理的输入空间的复杂度逐步提升：

1. **基于梯度的优化（Gradient-Based Optimization）**
   这类方法假设可以获取目标函数的梯度信息，并通过迭代方式不断优化参数。常见的技术包括**随机梯度下降（SGD）**和**牛顿法** \[801]。这些方法在连续可导的数值问题上非常有效，但由于要求函数可微，难以直接应用于离散任务，如提示词调优（prompt tuning）和带图结构的决策流程等结构化问题。

2. **零阶优化（Zeroth-Order Optimization）**
   这类方法**不依赖显式梯度信息**，而是通过目标函数的输出结果估计搜索方向 \[802]。典型方法包括**贝叶斯优化** \[803]、**进化策略** \[804] 以及**有限差分法** \[805]。这些方法适用于梯度不可得或计算成本高昂的情境。但它们依然依赖**明确定义的数值目标函数**和结构化的搜索空间，因此在面对基于语言的复杂任务时存在一定局限。

3. **基于大语言模型的优化（LLM-Based Optimization）**
   LLM 优化突破了数值函数的限制，能够在**结构化和高维输入空间**中开展优化任务。它们利用**自然语言**作为优化的输入空间和反馈机制，结合结构化推理与类人迭代能力，擅长对提示词进行细化、生成自适应工作流，并根据用户反馈不断提升任务性能。

---

尽管梯度法和零阶方法主要应用于数值目标，但它们的核心思想，如**迭代优化、搜索启发式、适应性学习**等，也为 LLM 优化策略提供了重要启发。在这些基础上，研究者们正快速发展出一类基于 LLM 的优化方法，结合了\*\*强化学习（Reinforcement Learning）\*\*的优势，成为“慢思维推理模型（slow thinking reasoning models）”的核心支撑 \[90, 806, 89]。

随着这些模型的不断演进，我们有理由相信，它们将推动下一波智能体应用浪潮，使得 LLM 能够在复杂环境中展现出更强的适应能力与策略性前瞻。

---

### 10.2 对大型语言模型优化的迭代方法

一些基于LLM的优化方法直接借鉴了经典优化理论，通过调整其关键组件来应对离散和结构化挑战。这些方法的核心特征是**迭代更新步骤**：模型生成一系列可能的改进选项，从中选择最佳修改来优化目标。以第9.1式中的提示优化目标为例，一个通用的迭代算法如下所示：

* **采样（Sample）**：T ∼ D
* **评估（Evaluation）**：L(T; Tp) ← ϕeval (ϕexe(Q, Tp), T)
* **更新（Update）**：T′p ← ϕopt (L(T; Tp))

其中，采样和更新步骤根据智能体的任务而定。在最简单的情形下，例如优化一个用于电影评论二分类任务的指令，目标L可以用分类准确率来衡量。在更复杂的智能工作流中，决策变量可能包括多个阶段的提示、工具选择、智能体拓扑结构或它们的组合。正如第9章所述，这些决策变量通常具有组合性质，例如：LLM词表V中的所有字符串集合，或一个工作流中所有智能体角色分配的可能性集合。由于穷举所有可能解是不可行的，因此需要设计近似的更新步骤ϕopt，接下来我们将讨论这些方法：

---

#### • 随机搜索（Random Search）

早期的LLM优化方法使用随机搜索变体来在离散的自然语言空间中优化提示词 \[774, 807, 651, 732, 808, 809, 810]。这些方法通常类似于进化算法，通过迭代采样候选决策变量，并从每一轮中选出表现最好的若干个。其通用形式如下：

* 采样：T ∼ D
* 评估：L^(i) ← ϕeval(ϕexe(Q, T\_p^(i)), T)，i = 1, ..., M
* 更新：{T\_p'^(k)}*{k=1}^K ← ArgTopK*{i∈\[M]} L^(i)
* 可选的补充步骤：{T\_p^(j)}*{j=K+1}^M ∼ Mutate({T\_p'^(k)}*{k=1}^K)

这里重定义M为每轮中采样的候选提示数，K（K < M）控制保留的最优提示个数。可选的**补充步骤**有助于在多轮迭代中保持候选池的多样性。随机搜索方法实现简单、易于并行化，尤其适用于单一提示工作流，此外在选择上下文示例方面也表现良好 \[811, 812]。但其效率代价较高——每轮迭代需要 O(M) 个并行API调用，这在复杂工作流中代价高昂。

---

#### • 梯度近似（Gradient Approximations）

一些方法尝试通过迭代方式逼近基于梯度的更新。例如，\[779, 730, 728] 在不同的工作流阶段生成改进内容。StraGO \[722] 使用中心差分启发式来估计下降方向，Trace \[813] 则将程序建模为计算图，通过类似反向传播的方法进行优化。

连续优化中梯度更新与提示空间中改进的类比核心在于“下降方向”——即对决策变量的一种系统性修改，以提升目标性能。与之相比，随机搜索在每轮中独立提出新决策变量，不考虑历史信息；而梯度法则利用历史轨迹，通常收敛更快。其通用迭代形式如下：

* 采样：T^(i) ∼ D，i = 1, ..., M
* 评估：L^(i) ← ϕeval(ϕexe(Q, Tp), T^(i))
* 梯度近似：g ← ∇LLM Agg(L^(1), ..., L^(M))
* 更新：T′p ← ϕopt(Tp, g)

其中，M 是小批量大小，Agg(·) 是聚合函数（在数值优化中通常是平均），∇LLM 表示一种“LLM梯度算子” \[728]，根据反馈信号和当前批次生成文本形式的改进方向（例如：“应考虑边界情况……”），ϕopt 可以是一个LLM查询，根据g进行提示更新。

相比随机搜索，梯度方法有两个优势：

1. 可以将历史改进方向整合进ϕopt，类似动量法（Momentum）\[814, 815]；
2. 便于在优化计算图时使用类似反向传播的技术 \[651, 813, 780]，尤其适用于多阶段、模块间依赖强的工作流。

但这些灵活性也意味着更高的设计开销，例如需要**元提示**来聚合反馈并应用更新方向。一些工作也研究了对“软提示”直接进行梯度优化 \[816, 817, 818]，虽然对简单的输入-输出学习效果不错，但在多步任务和顺序决策上表现不佳 \[630, 300]。

尽管这些方法借鉴了一阶优化思想，但将二阶技术（如拟牛顿法）拓展至LLM优化仍处于初步阶段。近期工作如 Revolve \[780] 迈出了这一步，引入结构化方法进行二阶优化，建模多轮响应模式的演化。通过引入高阶改进，Revolve 能实现更稳定和高效的优化，缓解复杂任务中的收敛停滞问题。我们也对推理阶段计算资源的利用趋势表示关注，这些资源可用于引入历史改进轨迹，甚至考察“动量”的作用 \[90, 89]。

---

#### • 贝叶斯优化与代理建模（Bayesian Optimization and Surrogate Modeling）

尽管上述方法在LLM优化中取得了显著进展，但它们通常伴随高昂的成本（财务和环境方面），因为需要大量的LLM调用。此外，这些方法对噪声较敏感，而提示等离散决策变量的优化空间仍不明确 \[819, 820]。在这些限制下，贝叶斯优化（BO）成为一个有吸引力的替代方案，因为它建立了一个对噪声鲁棒的目标函数代理模型：

* 采样：T ∼ D
* 提案：{T\_p^(i)}\_{i=1}^M ∼ S.Propose
* 评估：L^(i) ← ϕeval(ϕexe(Q, T\_p^(i)), T)，i = 1, ..., M
* 更新：S ← S.UpdatePrior({L^(i)}, {T\_p^(i)})

其中，S 表示一个概率代理模型，具备提案机制（如高斯过程BO的后验采样 \[803]）和基于提示评估结果的先验更新机制。例如，MIPRO \[821] 使用树结构Parzen估计器（TPE）\[822]作为代理模型，而 PROMST \[823] 则训练一个得分预测模型来指导提示调优。

利用代理模型进行LLM优化符合近年来对不可微目标进行“摊销优化”（amortized optimization）的趋势 \[824]。例如，\[825] 训练一个提示生成LLM来摊销Beam Search中查找破解提示前缀的计算成本。

此外，还有研究通过为LLM输出拟合一个轻量模块（如贝叶斯后验或效用函数）来辅助领域特定任务的优化，例如决策制定和多智能体协商 \[826, 827]。这类摊销方法，即拟合一个可复用的参数化模型来应对未见输入，正在LLM优化中获得越来越广泛的应用，包括破解安全防线等任务 \[828, 825]。


---

### 10.3 优化的超参数

与传统优化类似，基于大语言模型（LLM）的方法对影响搜索效率和泛化能力的超参数也非常敏感。在基于梯度的LLM优化器中，一个关键因素是聚合函数 Agg(·) 的选择，它决定了如何将文本反馈合成为用于指导提示更新的信息。不当的选择可能会导致关键信息的丢失，或在迭代优化过程中出现方向偏差。

此外，\[813] 引入了一种“白板”方法，将LLM程序分解为人类可解释的模块。但这种模块化工作流的结构设计仍大多未被深入研究，这为优化LLM驱动的决策流程带来了挑战。

LLM优化中的超参数往往与数值优化中的类似。例如，**批量大小（batch size）**至关重要：就像在传统优化中小批量更新提升了稳定性与效率，LLM方法如 TextGrad \[728] 也在进行更新前整合了多个生成样本的反馈。另一个关键因素是**动量（momentum）**——在梯度方法中通过结合历史梯度稳定更新，而LLM优化器同样利用历史优化过程来逐步提升性能 \[728, 813]。尽管数值优化取得了进展，但LLM优化器的超参数选择仍主要依赖经验性策略，通常通过反复试验进行调整。

在代理系统（agentic system）设计中，超参数广泛分布于多个组件，包括代理角色的分配、上下文示例的选择以及工具调用的调度等。每一个选择都会对最终性能产生深远影响，但目前仍缺乏系统的方法对其进行优化。尽管传统的超参数调优方法（如网格搜索、贝叶斯优化）可用于离散的LLM工作流，但由于语言模型输出的高方差，这些方法的计算成本非常高。

此外，这些超参数具有组合爆炸性，代理配置、提示策略与推理结构之间存在复杂交互，使得穷举搜索难以实现。近期研究尝试通过将代理式工作流嵌入结构化框架（如有限状态机 \[729]、最优决策理论 \[826]、博弈论 \[827]）来应对这一挑战，但这些方法往往难以在不同环境中泛化。

一个前景可期的方向是**元优化（meta-optimization）**，即使用LLM优化自己的超参数与决策策略。例如，LLM优化器可以将过去的决策作为经验，自我迭代优化提示策略，类似于深度学习中的学习优化器 \[829]。此外，\*\*摊销式方法（amortized approaches）\*\*通过训练辅助模型预测有效超参数，从而降低穷举搜索的成本 \[821, 823]。这些方法虽然充满潜力，但也引入了新的挑战，比如在自适应调优中平衡探索与利用，并确保对不同优化任务的泛化能力。开发适用于LLM工作流的系统性元优化策略仍是未来研究的关键方向。

---

### 10.4 跨深度与时间的优化

与在静态环境中更新参数的传统优化器不同，LLM能够动态优化工作流，既考虑“深度”（一次性流程）也考虑“时间”（循环迭代更新）。

在“深度”层面，LLM像前馈神经网络一样工作，随着流程经过不同模块而顺序优化——当前大多数LLM优化器都遵循这种方式。但LLM也能进行“跨时间”的优化，类似循环架构（如RNN或通用Transformer \[830]），通过多轮反馈不断改进决策。例如，StateFlow \[729] 融入多轮迭代反馈，使流程在时间维度上动态优化和适应。

尽管这些类比很有启发性，许多经典工程优化技术（如检查点机制 \[831] 和截断反向传播 \[832]）在LLM优化中仍未被充分探索，这一方向具有重要的研究潜力，也呼应了前文的研究呼吁 \[813]。

---

### 10.5 理论视角

最新研究表明，Transformer 模型在本质上就具备类优化计算的能力，支持其作为通用优化器的潜力。然而，其经验成功与理论理解之间仍存在显著差距。以下是一些缩小这一差距的研究进展概览：

* **上下文学习（In-Context Learning）**：一种核心视角是将Transformer视为优化器，尤其在少样本学习中。\[733] 显示Transformer可以在上下文中学习多种回归假设，包括正则化线性模型、决策树和浅层神经网络。后续研究 \[734, 833, 735] 进一步通过构造性证明说明Transformer能够实现梯度下降、二阶更新等迭代优化算法。然而，这些理论模型主要刻画了优化能力，但尚未完全解释大型LLM中离散输入输出形式的上下文学习机制。为此，一些实证分析 \[819, 834, 820] 试图理解预训练LLM在上下文中的泛化能力。\[834] 提出上下文学习类似隐马尔可夫模型（HMM）执行隐式贝叶斯推理，而 \[819, 820] 质疑“上下文示例相当于测试样本”的传统看法。上下文学习依然是LLM实现自我优化的核心能力，但其理论机制仍未被全面揭示。

* **机制可解释性（Mechanistic Interpretability）**：与理论分析并行，该领域试图通过识别特定行为对应的计算子图（即电路）来揭示Transformer内部计算过程。早期研究在GPT-2中分析了风格化语言任务的电路结构 \[836, 837, 838]；而近期则通过稀疏自编码器 \[839, 736, 840, 841] 寻找具有语义意义的特征。这些方法在激发LLM具备因果性和可控性方面取得了一定成果，但也暴露出一个副作用：在多样本提示下，上下文学习常将有益泛化与潜在有害行为混合在一起 \[842]，这对LLM工作流的安全与可靠优化构成挑战。

* **在不确定性下的局限性**：尽管LLM在有上下文支持时表现出一定的序列决策能力，但在不确定环境下仍难以做出最优决策 \[843, 844, 845, 846]。例如，\[826] 发现LLM优化器在处理随机性环境时难以适应，通常无法有效探索。这些结果提醒我们，在动态或不确定环境中部署LLM优化器需格外谨慎，尤其当任务对探索性和稳健性要求较高时。

LLM通过整合结构化推理、自然语言处理和上下文学习，正在重塑优化的定义，突破了传统数值优化方法的局限。尽管在结构化搜索空间中表现出强大的经验性能，但关于其理论基础，特别是上下文学习能力如何从标准的梯度训练中涌现出来，仍有许多未解之谜。

---




